<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Very的计算机学习</title>
    <link>https://meanvery.github.io/post/</link>
    <description>Recent content in Posts on Very的计算机学习</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>Copyright © 2008–2019, Steve Francia and the lee.so; all rights reserved.</copyright>
    <lastBuildDate>Tue, 08 Nov 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://meanvery.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>分割-CRIS</title>
      <link>https://meanvery.github.io/post/2022-11-08-%E5%88%86%E5%89%B2-cris/</link>
      <pubDate>Tue, 08 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>https://meanvery.github.io/post/2022-11-08-%E5%88%86%E5%89%B2-cris/</guid>
      <description>CRIS: CLIP-Driven Referring Image Segmentation ​	参考图像分割（Referring Image Segmentation，以下称其为“参考分割”）致力于研究分割出图片中符合语言描述的对象：给出词汇“人脸”，该研究就分割出图像中的人脸。
摘要（Abstract） ​	受到语言-图像对比预训练（Contrastive Language-Image Pretraining , CLIP）的启发，作者提出了 CRIS，设计了一个视觉语言解码器，将文本表示中细粒度的语义信息作用到每一个像素级别运算的激活上，增强了这两种模态的一致性。同时提出了文本到像素的对比学习方法，强迫文本特征和相关的像素级特征具有相似性，和不相关的像素级特征具有差异性。在无需任何后处理的情况下，CRIS 在三个基准数据集上的实验测试结果都显著优于之前的最佳结果。
1.介绍（Introduction） ​	与语义分割和实例分割不同的是，参考分割不局限于找出某个特定类别的对象，而是根据输入的语言表达找出对应的特定区域，显然如果这项技术发展成熟，可以用于交互图像编辑以及人机交互等领域。
​	但是由于语言文本和图像是不同的模态，各自具备不同的属性，因此很难使得文本特征与像素相匹配。不过得益于强大的深度学习，早期的方案是直接将文本特征和图像特征连接起来，使用合并得到的特征去生成分割的遮罩；随后，为了增强两种模态之间的交互，人们提出了一系列基于语言-图像注意力机制的方法来更好地学习跨模态的特征。
​	现在的方法则是利用外部给定知识加强学习效果，主要利用单模态预训练（预训练的图像或文本编码器），但是这就缺少了多模态间的关联信息。通过对大规模的无标注的数据进行语言监督，视觉语言预训练能够学习大量的多模态表示。CLIP 从 4 亿图片-文本对中学习到了图片级别的视觉概念，使得许多多模态任务取得了巨大的进展，包括图像-文本检索，视频-文本检索。但是，将 CLIP 直接应用到参考分割中，效果并不理想，如下图：
参考分割效果示意图​	​	因为图片级别预测和像素级别预测具有巨大的差异，前者关注图片的全局信息，而后者还需要学习细粒度即小区域的视觉表示。
​	在文章中，作者基于 CLIP 进行参考分割，尝试增强跨模态匹配的能力。考虑到参考分割的特点，作者提出了一个有效且灵活的框架——CLIP-Driven Referring Image Segmentation（CRIS），可以将 CLIP 得到的大量跨模态但是相关的知识用于完成文本-像素的匹配。
​	首先，作者提出了一个视觉语言解码器（visual-language decoder），一方面它可以通过自注意力机制，捕捉图片中距离较远但是相关的像素级特征；另一方面，它可以通过交叉注意力机制，自适应地将精细的文本特征融合到图片的像素级特征中。
​	其次，作者引入了文本-像素的对比学习，它可以在多模态空间中，将语言特征和相关的像素级特征匹配起来，同时分离不相关的像素级特征。基于这个方案中语言和像素级视觉特征的交互，模型可以明确学习到细粒度即小区域的视觉概念。
​	作者认为他们的研究主要有以下三个贡献：
提出了 CLIP-Driven Referring Image Segmentation（CRIS），将 CLIP 得到的知识用于匹配文本和像素； 创新地提出了”视觉语言解码器“、”文本-像素对比学习“来充分利用跨模态的知识； 在三个基准数据集上的实验结果显著优于之前的最佳水平。 2.相关工作（Related Work） 视觉-语言预训练（Vision-Language Pretraining） ​	利用语义监督下的大规模图片数据，人们提出了许多方法来通过文本表示学习到视觉表示。MIL-NCE 主要探索利用嘈杂的大规模的 Howto100M 教学视频，通过端到端的方式学习得到更好的视频编码器；SimVLM 利用大规模的弱监督来降低训练复杂性，采用单前缀语言以端到端的方式来建模目标。得益于从互联网中收集到了大规模的图像文本对，CLIP 在嵌入空间中对两个模态的表达式的匹配取得了引人瞩目的成功，它采用强大的语言模型和视觉特征编码器进行对比学习，捕捉显著的视觉特征实现零镜头（Zero-Shot）分类。最近一系列将 CLIP 模型所得知识用于下游任务（比如视频标题、视频-文本检索和图像生成）的工作都取得了卓越的成果。</description>
    </item>
    
    <item>
      <title>Seminar-20221017-BERT, ViT</title>
      <link>https://meanvery.github.io/post/seminar-20221021/</link>
      <pubDate>Mon, 24 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>https://meanvery.github.io/post/seminar-20221021/</guid>
      <description>感谢白宇师兄的讲解！
BERT12 Bert和GPT,ELMo 对比示意图​	BERT 可以真正结合上下文信息，而不想 GPT 那样只能接收上文信息，同时采用了“Masked Language Model”随机屏蔽一些单词，以这个作为预训练目标，缓解单向语言模型的约束。
​	BERT 和之前的模型类似之处在于都采用了两个阶段，首先是 pre-training，然后是 fine-tuning。在 pre-training 阶段，BERT 在没有标注的数据上进行无监督学习，在 fine-tuning 阶段，BERT 首先利用预训练得到的参数初始化模型，然后利用下游任务中，标记好的模型进行带监督的学习，并对所有的参数进行微调。
​	为了能够处理各种任务，BERT 给出了句子级别的表示方案，包括单个句子和句子对，同时在 BERT 中，句子实际上指的是任意长度的连续文本，而不是语法意义上的某个句子。
​	BERT 的编码采用的是 WordPiece。每个序列的开头都是一个特殊的类别标记 CLS。区分句子对中前后两句的方法：首先是用特殊的分隔符标记 SEP 把不同的句子隔开，然后在每个标记中加入一个学习得到的编码，以此来显示该标记是属于句子队中的句子 A 还是句子 B。
​	最终一个标记的表示其实是由 3 个部分组成，1 是标记本身的含义编码，2 是显示属于哪个句子的编码，3 是学习得到的位置编码，如下图：
BERT 输入编码示意图​	​	BERT 采用了两种非监督任务来进行预训练，一个是 token-level 级别的屏蔽模型（Masked LM），一个是 sentence level 级别的 Next Sentence Prediction。BERT 的损失函数也是两个任务的损失函数之和。
Masked Language Model ( MLM )
其实在类似完型填空的任务中，如果直接使用双向语言模型，那么模型就可以间接知道要预测的单词，所以为了解决这个问题，谷歌提出了 Masked LM，随机屏蔽一些 token，并通过上下文预测这些 token。在具体的实验中，BERT 会随机屏蔽每个序列中 15% 的 token 并用 mask token 来代替。但是真实的任务中以及在进行参数的微调时，没有 mask token，所以为了缓解 mask 的影响，谷歌的方法是：选中 15% 的标记，准备进行 mask，在这 15% 当中，选择 80% 直接 mask 掉， 选择 10% 赋予随机的标记，最后 10% 保持原来的标记不变。然后这 15% 的位置将在交叉熵损失函数的控制下，来用于预测原本的值。</description>
    </item>
    
    <item>
      <title>Seminar-20221010-Self Attention, Transformer</title>
      <link>https://meanvery.github.io/post/seminar-20221010/</link>
      <pubDate>Tue, 11 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>https://meanvery.github.io/post/seminar-20221010/</guid>
      <description>感谢陈露元同学、胡占斌同学的讲解！
Self Attention ​	首先是应用在语言处理领域，对词语进行位置编码，然后经过自注意力机制算法，就可以自动得到具备相关性的计算结果。这种相关性使得该结构可以掌握关联信息，捕捉到距离较长的词语之间的关系，使得翻译任务得到了更好的处理。
位置编码
在我看来，后续的计算其实是对编码的计算，那么就要保证编码尽可能全面地反应了对象的信息，同时还具备一些其他的功能，比如可以反映编码之间的关联。所以编码其实也是一个需要仔细斟酌的问题，实际上也经历了较多过程的演化。
用整型值标记位置
第一个位置标记 1，第二个位置标记 2……以此类推。
这种方式的缺点是，模型应用时遇到的序列可能比训练时所用的序列长，导致模型的泛化性能较差。其次，这种编码方式会让编码随着序列长度的增加越变越大，没有一个确定的界限。
用 [0,1] 范围标记位置
有多少个位置就在该区间内等距离地取多少个点的值。
这种方式下，当序列的长度不同时，位置之间的相对距离会变化，数值变化使得模型对位置之间的关系的理解出现偏差。
用二进制向量标记位置
缺点是编码出来的位置向量，其实在空间内是离散的，不同位置之间的变化不连续，会产生数学计算上的不方便。
用周期函数 sinx/cosx 标记位置
低位给予较小的周期，高位给予很大的周期，这样随着序列长度的增加，高位变动缓慢，低位变动迅速。同时可以使得位置向量的值是有界的且位于连续空间，模型在处理时就更容易泛化。并且还可以通过简单的计算，得到两个词向量的距离关系，有助于衡量词之间的关联。
自注意力机制
将得到的位置编码矩阵经过计算后，分别又得到 Q、K、V 矩阵，借助注意力公式： $$ Attention(Q,K,V)=softmax(QK^T/\sqrt d_k)V $$ 计算出每个词的注意力，相当于对初始的词向量，加入了整句话中与之有关的信息，重新优化了每个词的编码，使之内涵更加丰富。
Transformer1 Transformer 结构示意图​	encoder 对输入的内容进行位置编码，然后利用了多头注意力机制，即使用多个矩阵相乘，得到多组 Q、K、V 矩阵再进行后续的计算。Transformer 中也利用了残差的思想，将处理前的信息直接传递到经过多头注意力机制处理后的位置，通过加法融合在一起，再进行 LN，即层正则化，因为每一层代表了一个句子，层正则化才可以处理到一句话之类的信息，如果还是应用 BN 来处理，不同句子相对位置相同的词向量之间其实不具有有效的信息。然后把融合正则化完的结果进行前向传播，再次融合并正则化后作为一部分传递到 decoder。
​	decoder 实际上对训练时需要输出的数据，进行了位置编码并输入模型，按照同样的多头注意力机制算法进行计算，将输出的结果，结合 encoder 传递过来的部分，再进行一次注意力机制计算，就得到了输入和输出之间的注意力关系。最后经过一个利用了残差思想的前向传播网络，经过线性整合与 softmax 之后，得到输出结果的概率值。
https://arxiv.org/pdf/1706.03762.pdf&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description>
    </item>
    
    <item>
      <title>Seminar-20221007-PAN, DANet</title>
      <link>https://meanvery.github.io/post/seminar-20221007/</link>
      <pubDate>Sat, 08 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>https://meanvery.github.io/post/seminar-20221007/</guid>
      <description>感谢周羿旭老师的讲解！
PAN1 ​	场景中的文字检测一直是人们感兴趣的任务，近年来也取得了不错的发展，但是依然存在两个较大的困难阻碍了该技术在实际生活中的运用：一是速度和精度不平衡；二是对复杂形状文字的建模很困难。
PAN思路示意图​	Pixel Aggregation Network （PAN）的思路是，用分割网络预测文字所在的区域，并得到“核参数”以及相似向量，然后在预测的核中重建完整的文字实例，最后可以预测分割任意形状的文字实例。PAN 整体结构示意图​	输入图片后，利用一个轻量化的网络结构对原始图片进行卷积运算，得到不同尺度的特征图，然后将不同尺度的特征图的通道数统一减小为某个值，接着通过特征金字塔强化单元（Feature Pyramid Enhancement Module）计算，再通过特征融合单元（Feature Fusion Module）进行特征融合，得到最终的特征图，再从该特征图中得到想要的分割区域、核参数、相似向量。
DANet2 ​	Dual Attention Network（DANet）双重注意力网络，特点是自适应地集成局部特征和全局依赖，在传统的扩张 FCN 上附加了两种类型的注意力模块，分别模拟空间和通道维度中的语义依赖性。
位置注意力模块
通过所处位置处的特征加权来选择性地聚合每个位置的特征，无论距离如何，类似的特征都会被关联起来。
通道注意力模块
整合所有通道映射间的相关特征来选择性地强调存在相互依赖的通道映射。
​	将以上两个模块的输出相加来改进特征的表示，得到了更精确的分割结果。
DANet 整体结构示意图​	reshape 表示将矩阵展开成一维向量，transpose 表示进行转置操作。其余过程则完全符合注意力机制的运算过程。最后将两个模块的结果融合成最终的特征图，再计算得到分割结果。
https://arxiv.org/pdf/1908.05900.pdf&amp;#160;&amp;#x21a9;&amp;#xfe0e;
https://arxiv.org/pdf/1809.02983.pdf&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description>
    </item>
    
    <item>
      <title>Seminar-20221003-RefineNet, HRNet</title>
      <link>https://meanvery.github.io/post/seminar-20221003/</link>
      <pubDate>Tue, 04 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>https://meanvery.github.io/post/seminar-20221003/</guid>
      <description>感谢周羿旭老师的讲解！
RefineNet1 ​	反卷积（转置卷积）是上采样的一种方式，但是不能恢复浅层的特征，在 DeepLab 中，研究人员使用了空洞卷积来解决这一问题，但是空洞卷积也有两个问题：一是对高像素的特征图，卷积运算会消耗大量的运算资源，因此高像素图通常会经过 resize 变为低像素图；二是空洞卷积的特性，使得在计算过程中丢失了一些连续性的细节信息。
RefineNet 示意图​	如上图所示，RefineNet为了利用浅层的特征信息，卷积计算出小尺寸特征图后，还原为较大尺寸的特征图，再和较大的特征图连接在一起作为新的特征图，来进行最后的分割预测。
HRNet2 ​	为了使得分割目标的位置信息尽可能的精确，其中一个做法就是维持高分辨率的特征图，RefineNet 就时通过下采样（卷积）得到深层的强语义信息，然后再上采样恢复到高分辨率的浅层特征图，HRNet 认为这样的做法会使得大量的有效信息在上下采样的过程中丢失。于是研究人员提出了以下结构：
HRNet 示意图​	如上图所示，HRNet 并行了多个分辨率的分支，同时在分支之间同样有信息交互，达到了深层语义特征和浅层位置特征等兼得的目的。
https://arxiv.org/pdf/1611.06612.pdf&amp;#160;&amp;#x21a9;&amp;#xfe0e;
https://arxiv.org/pdf/1908.07919.pdf&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description>
    </item>
    
    <item>
      <title>Seminar-20220921-FCN, UNet</title>
      <link>https://meanvery.github.io/post/seminar-20220921/</link>
      <pubDate>Tue, 27 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://meanvery.github.io/post/seminar-20220921/</guid>
      <description>感谢高丹阳师姐的讲解！
FCN1 ​	Fully Convolutional Networks 将网络结构中的全连接层全部替换为了卷积层。
​	前面的结构采用的是传统的 CNN 结构，但是最后把全连接层替换为卷积层，生成多通道的特征图，每一个通道代表了被分为的类别，并可以计算出相应的损失函数。
​	网络原本的全连接层不能接收任意尺寸的特征图，但是在修改为卷积层后，则可以接收任意尺寸的输入特征图了。同时由于最后还是在进行卷积操作，因此可以利用插值等上采样方法将经过池化和卷积被缩小的特征图恢复到原图的尺寸，形成在原图上的像素级预测。
UNet2 ​	UNet 的应用场景是针对医学图像分割的，而医学图像的特点之一是样本量较少。 UNet实际上通过自身的卷积生成了额外可以用于计算的样本，同时样本之间信息可以互补，因此实现了较好的分割效果。
​	UNet 的前半段和一般的网络一样进行卷积特征提取，经过几次卷积池化后得到较小的特征图，然后利用上采样，将小的特征图又逐步恢复到原始尺寸。在这两个过程中，某些层的特征图大小是一样的，因此将大小一样的特征图结合起来，就得到了更多的可以用于预测的样本。最终的预测就是在这些原始卷积特征图，和卷积上采样特征图上进行的。
https://arxiv.org/pdf/1411.4038.pdf&amp;#160;&amp;#x21a9;&amp;#xfe0e;
https://arxiv.org/abs/1505.04597&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description>
    </item>
    
    <item>
      <title>Seminar-20220914-YOLO, SSD</title>
      <link>https://meanvery.github.io/post/seminar-20220914/</link>
      <pubDate>Tue, 20 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://meanvery.github.io/post/seminar-20220914/</guid>
      <description>感谢周羿旭师兄的讲解！
YOLO1 YOLO 模型示意图​	YOLO的模型相当简单，仅仅是一个卷积网络，同时预测多个边界框及其对应的类别概率。YOLO可以进行端到端的训练，在保持实时级速率的同时也维持了较高的平均准确度。
​	如上图，YOLO将输入图片的分辨率变为 \(448 \times 448\)，经过卷积运算后，在 \(7 \times 7\)的图中进行边框与类别预测， \(7 \times 7=49\)个像素格子，如果物体的中心落在某个像素格子内，那么这个格子就负责预测该物体。格子预测一组值，包含了边界框的坐标，边界框的高度和宽度，边界框内物体的类别概率（可能的类别数为 20，因此有 20 个概率值）。因此最后输出，包含的值一共有\(7 \times 7 \times (4+20)\)个。
​	YOLO的设计也有如下几个新颖的地方
激活函数
在网络的最后一层，研究人员用了对数激活函数，在其他层，用了如下的漏整流线性激活函数（Leaky Rectified Linear Activation）: $$ \phi(x) = \begin{cases} 1.1x &amp;amp;\text{if } x &amp;gt;0 \ .1x &amp;amp;\text{otherwise } \end{cases} $$
损失函数
作者考虑到，定位误差和分类误差对于总误差的影响应该是不同的，宁愿定位偏差大一点，不可以分类错误。由此给两种误差加上系数 \(\lambda\)来设置对总误差的影响。
总体形式采用了平方和误差（Sum-squared Error）。作者又考虑到，对于小边界框和大边界框，产生同样距离的偏差影响程度是不一样的，对小边界框的影响更大。为了缓解这个问题，求解损失函数时，代入的是边界框高度和宽度的平方根而不是其原有值。
另外，没有物体的边界框，不应该求解其损失函数，所以引入了判定函数\(\delta\)，如果没有物体，函数取值为0，有物体则为1。
最终得到的损失函数如下： $$ \sum_{\substack{0\leq i\leq 48}} \bigg(\lambda \delta \big((x_i-\tilde{x_i})^2+(y_i-\tilde{y_i})^2+(\sqrt{w_i}-\sqrt{\tilde{w_i}})^2+(\sqrt{h_i}-\sqrt{\tilde{h_i}})^2\big)+\sum_{\substack{c\in class}}(p_i(c)-\tilde{p_i}(c))^2\bigg) $$
YOLO也有以下缺点：
如果两个物体的中心落在同一格子中，则YOLO只能预测其中一个物体（给其中一个物体较大的概率分数）； 很难推广到具有新的或不寻常的纵横比或配置的对象； 大边界框中的小偏移通常影响较小，但小边界框中的小偏移对IoU的影响很大。YOLO主要的误差来源于定位的不准确。 SSD2 ​	SSD的核心设计理念有以下三点：</description>
    </item>
    
    <item>
      <title>Seminar-20220917-RetinaNet, FCOS</title>
      <link>https://meanvery.github.io/post/seminar-20220917/</link>
      <pubDate>Tue, 20 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://meanvery.github.io/post/seminar-20220917/</guid>
      <description>感谢高丹阳师姐的讲解！
RetinaNet1 ​	针对现有单阶段法 (one-stage) 目标检测模型中前景 (positive) 和背景 (negatives) 类别的不平衡问题，研究者提出了一种叫做 Focal Loss 的损失函数。为了检测提出的 Focal Loss 损失函数的有效性，所以作者提出了一种简单的模型 RetinaNet。
​	RetinaNet 的特征提取选择了残差网络 ResNet，特征融合选择了 FPN 结构。
Focal Loss 损失函数 $$ FL(p_t)=-(1-p_t)^\gamma log(p_t) $$ \(p_t\) 是由模型预测的属于前景的概率 p 得来，如果是前景则 \(p_t\) 取值为 \(p\) ，否则取值为 \(1-p\) 。 \(\gamma \)是一个范围在 [0,5] 的参数。
当 \(p_t\) 趋向于 1 时，说明该样本容易区分，调制因子 \((1-p_t)^\gamma \) 趋向于 0，即对 Loss 的贡献会很小；如果某个样本被错分，则 \(p_t\) 很小，此时调制因子 \((1-p_t)^\gamma \) 趋向于 1，相比最原始的损失函数，对 Loss 的影响减小。
同时参数 \(\gamma \) 能够调整权重衰减的速率，经过实验验证后得到当 \(\gamma =2 \)时，效果最好。</description>
    </item>
    
    <item>
      <title>Seminar-20220903-R-CNN, FPN</title>
      <link>https://meanvery.github.io/post/seminar-20220903/</link>
      <pubDate>Mon, 05 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://meanvery.github.io/post/seminar-20220903/</guid>
      <description>感谢赵永瑞同学的讲解！
R-CNN1 ​	因为研究者结合了区域提名（Region Proposals）和卷积神经网络（CNN），因此将其命名为 R-CNN。
R-CNN 基本流程示意图​	由上图可知，在测试时，首先输入图片，依据选择性搜索（Selective Search）算法，得到大约 2000 个类别无关的区域提名；然后将提名的全部区域的边界，向外扩张 16 个像素，接着无论其大小或者纵横比怎么不同，都缩放到同一尺寸（ \(227 \times 227\) ）；再利用 CNN 提取出特征向量，其中包含了 5 个卷积层和 2 个全连接层，提取到的特征向量维度为 4096；最后通过线性支持向量机对区域进行分类。
​	此外还面临着带标签数据不足，因此大型 CNN 难以训练的问题；在研究者提出的模型中使用了，在大型辅助数据集（ILSVRC）上进行带监督的预训练，然后再在小数据集（PASCAL）上进行参数等微调的方式。结果表明在数据有限时，这是一种相当有效的方式。
​	在 R-CNN 后续研究中，研究人员发现只需要一个简单的边框回归方法，就可以让错误定位显著减少。
FPN2 ​	识别图片中不同尺度的物体一直是计算机视觉中一个重要的任务。
多尺度融合发展示意图​	图片金字塔（Feature Pyramids）为标准化方法提供了基本的思路，如上图中 （a）：将一张图片变换为不同尺寸，然后在每种尺寸上，进行卷积运算特征提取并进行预测。但是这种方式一张图片就要当作多张图片去训练和计算，时间成本太高。
​	改进的方法如上图（b），对一张图片进行卷积操作，然后仅利用多次卷积后得到的深层语义特征进行预测。速度较快，但是没有用到浅层的信息，特征图分辨率也低，不能准确包含物体的位置信息。
​	进一步改进如上图（c），对一张图片进行卷积操作，从每次卷积后的特征图上进行预测，虽然不需要额外的计算，但是单一尺度的语义特征不够丰富。
​	如上图（d），为了融合不同尺度的特征，研究人员提出了 Feature Pyramid Network 即 FPN。
​	在 FPN 中，主要包含了三个过程：自底向上，自顶向下，横向连接。
FPN 示意图自底向上（Bottom-Up Pathway）
将原始图片输入 CNN 进行特征提取，经过卷积层时，因为卷积核和步长等设置，有的输出图与输入图一致（这些卷积层归为一个 stage ），有的输出图缩小为原来的 \(1/2\) （划分到下一个 stage ）。将每个 stage 中的最后一层输出的特征抽取出来（最后一层经过了最多的计算，语义最丰富），由于第一个 stage 的输出特征图占用内存过大，因此舍弃。最后不同 stage 的输出图尺寸其实与原图尺寸依次保持着 4，8，16，32 倍的关系。</description>
    </item>
    
    <item>
      <title>Seminar-20220831-ShuffleNet, EfficientNet</title>
      <link>https://meanvery.github.io/post/seminar-20220831/</link>
      <pubDate>Thu, 01 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://meanvery.github.io/post/seminar-20220831/</guid>
      <description>感谢周羿旭师兄的讲解！
ShuffleNet1 2 3 4 ​	研究人员注意到，由于计算代价颇大的密集 \(1 \times 1\) 卷积，Xception 和 ResNeXt 等最先进的架构在规模很小的网络中效率较低。于是使用了逐点组卷积来降低 \(1 \times 1\) 卷积的计算复杂度。 同时为了克服组卷积带来的副作用（通道间信息交流问题），研究人员提出了一种新颖的通道混洗操作（shuffle）来帮助信息在特征通道之间流动。 ​	基于这两种技术，他们构建了一个称为 ShuffleNet 的高效架构。 与当初流行的结构相比，对于给定的计算复杂度预算，ShuffleNet 允许更多的特征图通道编码更多信息，同时使得规模非常小的网络的性能也能达到较好的水平。
​	在规模较小的网络中，计算代价昂贵的点卷积（pointwise convolution）导致了，如果想要保持较低的计算复杂度，就必须减少通道数量，然而这会使得准确率显著下降。为了解决这个问题，就只有进行稀疏连接（sparse connection），每个卷积操作仅作用于相关的输入通道组，即分组卷积，但是这又会阻碍通道间的信息交流，并因此弱化特征表达能力。
​	所以在稀疏连接的基础上，还需要让分组卷积获得其他组的数据。由此想到了 channel shuffle 操作，在每个通道的基础上，再进行分组，得到通道的子序列，将这些子序列重新有规则地组合，形成下一层的输入。
channel shuffle 示意图ShuffleNet Unit的设计EfficientNet5 6 7 8 ​	扩张卷积网络被广泛用于追求更高准确度，比如使用更多的层使得 ResNet-18 变为 ResNet-200。但是扩张卷积网络的过程其实还没有被人们琢磨清楚，现在也有许多方式来完成扩张，比如增加深度，宽度，分辨率。尽管人们可以随意选择维度对网络进行个性化的扩张，但是随意地调整后，需要繁复的参数调节，并且效果（准确率，效率等）并不一定优良。
​	因此研究人员重新学习反思了卷积网络扩大规模的过程，并提出了关键性的问题——是否有一个标准化的方法去扩张网络，使之总能得到更好的准确率和效率。在实验过程中，研究人员发现，需要去平衡网络的深度，宽度和分辨率。但是出乎意料的是，不需要非常复杂的数学函数关系，这种平衡可以通过简单地以恒定比例缩放其中每一个维度来实现。
​	在文章中，研究人员提出了复合缩放算法（Compound Scaling Method），使用了复合因子 \(\phi\) 来规范地缩放宽度，深度和分辨率： $$ \alpha \geq 1,\beta \geq1,\gamma \geq1 $$
$$ depth:d=\alpha^\phi $$
$$ width:w=\beta^\phi $$</description>
    </item>
    
    <item>
      <title>Seminar-20220827-SENet, MobileNet</title>
      <link>https://meanvery.github.io/post/seminar-20220827/</link>
      <pubDate>Sat, 27 Aug 2022 00:00:00 +0000</pubDate>
      
      <guid>https://meanvery.github.io/post/seminar-20220827/</guid>
      <description>感谢张若淇师兄的讲解！
SENet1 2 3 ​	在之前的研究中，人们重点关注空间维度上特征的联系与交互，但是 SENet 研究了网络设计的不同方面——通道（channel）之间的关系。SENet 设计了一个新的结构单元，称之为 Squeeze-and-Excitation（SE）块，它是为了建立特征图通道之间的关系，并以此来提升特征表示的效果。
A Squeeze-and-Excitation block​	其中， \(X\) 是初始的特征图，高度宽度通道数分别为 \(H&#39;\) ， \(W&#39;\) ， \(C&#39;\) 。
​	\(F_{tr}\) 是传统的卷积操作将 \(X\) 变换为 \(U\) ，高度宽度通道数分别为 \(H\) ， \(W\) ， \(C\) 。
​	然后使 \(U\) 经过 Squeeze 操作即 \(F_{sq}\) ，变换成 \(1 \times 1 \times C\) 的通道描述向量 \(z\) 。实际上是用全局平均池化，将每一通道上原本为 \(H \times W\) 的二维矩阵通过求平均的方式，化为一个实数，这个实数其实一定程度上包含了每一通道上的全局视野。同时，化成 \(1 \times 1 \times C\) 也避免了高度和宽度（也即空间特征）带来的影响，从而突出了通道的作用。
​	再让 \(1 \times 1 \times C\) 的通道描述向量 \(z\) 经过 Excitation 变换，实际上经过了两个全连接层。</description>
    </item>
    
    <item>
      <title>Seminar-20220824-ResNet, DenseNet</title>
      <link>https://meanvery.github.io/post/seminar-20220824/</link>
      <pubDate>Wed, 24 Aug 2022 00:00:00 +0000</pubDate>
      
      <guid>https://meanvery.github.io/post/seminar-20220824/</guid>
      <description>感谢陈露元同学的讲解！
ResNet1 2 ​	从之前已有的研究中可以看出，似乎神经网络的深度越深，学习效果就越好。但这个结论其实并不可靠：一个问题是，随着深度的增加而出现的梯度消失或梯度爆炸现象，影响最后的收敛，不过这种收敛问题可以通过正则化（regularization）得到部分解决；另一个问题是，网络即使收敛，在深度增加时，正确率却无法进一步提高甚至还会下降，称为网络的退化（degradation）问题。
​	考虑一个极端的情形，如果增加的所有层都是前一层的直接复制，那么深层网络的训练误差应当和浅层网络相等，因此网络退化的根本原因是优化的问题，不是所有的系统都很容易优化。
​	为了解决优化的难题，提出了残差网络，不让网络直接拟合原先的映射，而是拟合残差映射。
Residual learning: a building block​	上图为残差网络中的一个模块，identity mapping 为恒等映射。由上图，残差网络可以理解为在前向网络中增加了一些快捷连接（shortcut connections），这些连接会跳过某些层，直接将原始数据传到之后的层。新增的快捷连接也不会增加模型的参数和复杂度，整个模型依然可以使用端到端的方法进行训练（比如随机梯度下降法）。
不同层数的ResNet架构总结 ​	ResNet 将部分原始输入信息不经过矩阵乘法和非线性变换，直接传输到下一层，快捷连接如同在深层网络中建立起了信息的捷径。通过改变学习目标，残差网络不再学习完整的输出而是学习残差，不仅降低了学习的难度，而且解决了传统卷积层或者全连接层在信息传递时存在的丢失和损耗问题——直接将信息从输入绕道传到输出，一定程度上保护了信息的完整性。
ResNet.version23 4 对resnet的改良​	实验表明，保持一个单纯的信息传送通道（如上图中 b 所示）对降低优化的难度有帮助。
残差网络的分析
原始版本的残差模块，利用公式描述如下： $$ y_l = h(x_l)+F(x_l,W_l)&amp;hellip;(1) $$
$$ x_{l+1} = f(y_l)&amp;hellip;(2) $$
其中，\(x_l\)为第\(l\)层残差模块的输入特征，\(W_l\)是权重系数，\(F\)是残差函数，\(f\)是ReLU函数，\(h\)是恒等映射，即\(h({x_l}) = x_l\)。
如果我们设\(f\)也是恒等映射，即\(x_{l+1} = y_l\)，则可将（2）式代入（1）式，可得： $$ x_{l+1} = x_l + F(x_l,W_l)&amp;hellip;(3) $$ 类似的可推得： $$ x_L= x_l + \displaystyle\sum_{i=l}^{L-1}F(x_i,W_i)&amp;hellip;(4) $$ 由（4）式可得任意深层特征都可以用浅层特征加上残差项来表达，而对原始 ResNet，深层特征实际是各项相乘。
对损失函数求导可得： $$ \dfrac{\partial \varepsilon}{\partial x_l}=\dfrac{\partial \varepsilon}{\partial x_L}\dfrac{\partial x_L}{\partial x_l}=\dfrac{\partial \varepsilon}{\partial x_L}\Big(1+\dfrac{\partial}{\partial x_l}\displaystyle\sum_{i=l}^{L-1}F(x_i,W_i)\Big)&amp;hellip;(5) $$ 由（5）式可得梯度被分解为两项，第一项确保了信息可以直接传播回任意浅层特征，第二项确保了即使权重任意小，层的梯度也不会消失。</description>
    </item>
    
    <item>
      <title>Seminar-20220820-AlexNet, GoogLeNet, VGGNet</title>
      <link>https://meanvery.github.io/post/seminar-20220820/</link>
      <pubDate>Mon, 22 Aug 2022 00:00:00 +0000</pubDate>
      
      <guid>https://meanvery.github.io/post/seminar-20220820/</guid>
      <description>感谢高丹阳学姐的讲解！
AlexNet 首次使用GPU进行模型训练 使用ReLU激活函数代替Sigmoid/Tanh 使用规范层(Local Response Normalization, LRN) 使用随机丢弃策略(Dropout) GoogLeNet12 Inception(也称为GoogLeNet)结构的核心思想与贡献：
使用 \(1\times1\) 的卷积来进行升降维 在相同尺寸的感受野种叠加更多的卷积，能提取更丰富的特征。同时在卷积之后都跟着激活函数，多层卷积与激活也有助于组合出更多的非线性特征 降低了计算的复杂度，对输入降维后再做卷积计算量明显减小 在多个尺寸上同时进行卷积后再进行聚合 直观上在多个尺度上同时进行卷积，能提取到不同尺度的特征，特征更丰富也意味着最后分类判断时更加准确 利用稀疏矩阵分解成密集矩阵计算的原理来加快收敛速度 Hebbin赫布原理，&amp;ldquo;fire together, wire together&amp;rdquo; : 两个神经元或者神经元系统，如果总是同时兴奋，就会形成一种组合，其中一个神经元的兴奋会促进另一个的兴奋 GoogLeNet网络深度深，为了避免梯度消失，网络额外增加了两个辅助的Softmax用于向前传导梯度。
VGGNet3 采用连续的几个 \(3\times3\) 卷积核代替AlexNet中的较大卷积核，即&#34;金字塔方法&#34;。金字塔方法示意图优点 结构简洁，整个网络都使用了相同大小的卷积核尺寸( \(3\times3\) )和最大池化尺寸(\(2\times2\)) 几个小卷积核的组合比一个大卷积核效果更好 验证了加深网络结构可以提升性能 缺点 使用了更多参数，耗费更多计算资源，导致更多的内存占用 https://zhuanlan.zhihu.com/p/32702031&amp;#160;&amp;#x21a9;&amp;#xfe0e;
https://blog.csdn.net/guoyunfei20/article/details/78395500&amp;#160;&amp;#x21a9;&amp;#xfe0e;
https://zhuanlan.zhihu.com/p/41423739&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description>
    </item>
    
    <item>
      <title>An Overview of Computer Vision</title>
      <link>https://meanvery.github.io/post/20220711/</link>
      <pubDate>Mon, 11 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>https://meanvery.github.io/post/20220711/</guid>
      <description>Computer Vision1 Image Classification(图像分类)：将图片按内容分到某一类别 Object Detection(物体检测)：检测出图片中的目标对象 Segmentation(图像分割)：将图片按内容分割 Semantic Segmentation(语义分割)：不同物体分成不同的部分 Instance Segmentation(实例分割)：同一物体要分为不同个体 Image Generation(图像生成) 1 Neural Network 1.1 Forward Propogation(前向传播) The input data should be fed in the forward direction only. 神经元通过加权和与激活函数决定是否进一步传递数据。 常见激活函数：Sigmoid, ReLU, Softmax, Hyperbolic Tangent
1.2 Back Propogation(反向传播) It is the practice of fine-tuning the weights of a neural net based on the error rate obtained in the previous epoch.
1.3 Several Loss Function(损失函数) Mean Absolute Error(平均绝对误差) Mean Square Error(均方误差) Hinge Loss(铰链损失) Cross Entropy Loss(交叉熵损失) 1.</description>
    </item>
    
    <item>
      <title>format guide</title>
      <link>https://meanvery.github.io/post/format_rules/</link>
      <pubDate>Mon, 11 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>https://meanvery.github.io/post/format_rules/</guid>
      <description>小标题 字体从大到小六个级别，H1最大，H6最小
H1 H2 H3 H4 H5 H6 这里的内容将在黑线 | 之后显示 这里同上
bold
斜体格式
灰色色块显著
引用格式1
表格项1 表格项2 内容1 内容2 内容3 内容4 反引号法黑色背景，通常用于代码块 xxxx&amp;quot;四空格法&amp;quot;黑色背景xxxx大括号法 黑色背景 编号 有序号的编号 第一 第二 第三 无序号（小圆点） 内容 内容 内容 嵌套编号 内容 嵌套1 嵌套2 嵌套3 内容 嵌套1 嵌套2 其他格式 缩写
H下标O
X上标
输入内容
黄色块高亮显示
新起一行
the above quote is excerpted from xxxxxx [article] (https://www.baidu.com) during xxxxxxx,日期&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description>
    </item>
    
    <item>
      <title>first try</title>
      <link>https://meanvery.github.io/post/try/</link>
      <pubDate>Sun, 10 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>https://meanvery.github.io/post/try/</guid>
      <description>how to use this</description>
    </item>
    
    <item>
      <title>Markdown Syntax Guide</title>
      <link>https://meanvery.github.io/post/markdown-syntax/</link>
      <pubDate>Mon, 11 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://meanvery.github.io/post/markdown-syntax/</guid>
      <description>&lt;p&gt;This article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Rich Content</title>
      <link>https://meanvery.github.io/post/rich-content/</link>
      <pubDate>Sun, 10 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://meanvery.github.io/post/rich-content/</guid>
      <description>&lt;p&gt;Hugo ships with several &lt;a href=&#34;https://gohugo.io/content-management/shortcodes/#use-hugos-built-in-shortcodes&#34;&gt;Built-in Shortcodes&lt;/a&gt; for rich content, along with a &lt;a href=&#34;https://gohugo.io/about/hugo-and-gdpr/&#34;&gt;Privacy Config&lt;/a&gt; and a set of Simple Shortcodes that enable static and no-JS versions of various social media embeds.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Placeholder Text</title>
      <link>https://meanvery.github.io/post/placeholder-text/</link>
      <pubDate>Sat, 09 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://meanvery.github.io/post/placeholder-text/</guid>
      <description>&lt;p&gt;Lorem est tota propiore conpellat pectoribus de pectora summo.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Math Typesetting</title>
      <link>https://meanvery.github.io/post/math-typesetting/</link>
      <pubDate>Fri, 08 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://meanvery.github.io/post/math-typesetting/</guid>
      <description>&lt;p&gt;Mathematical notation in a Hugo project can be enabled by using third party JavaScript libraries.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Emoji Support</title>
      <link>https://meanvery.github.io/post/emoji-support/</link>
      <pubDate>Tue, 05 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://meanvery.github.io/post/emoji-support/</guid>
      <description>&lt;p&gt;Emoji can be enabled in a Hugo project in a number of ways.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
