<!DOCTYPE html>













<html lang="en">
  <head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />

  
  <title>分割-发现会动的物体 - Very的计算机学习</title>

  
  
  <meta name="description" content="Discovering Objects that Can Move 摘要（Abstract） ​	本文研究了物体发现问题——在没有手动标签的情况下将对象从背景中分离出来。现有的方法利用外观的线索，如颜色、纹理和位置，将像素聚类成类似物体的区域。然而，仅依靠外观，这些方法无法在杂乱的场景中将物体与背景分离。这是一个基本的限制，因为对象的定义本质上是模糊的、上下文相关的。为了解决这种模糊性，本文选择关注动态物体——可以在世界中独立移动的实体。
​	作者将最新的基于自动编码器的框架，从玩具合成图像扩展到复杂现实场景的无监督物体发现。为此，简化了它们的结构，并使用来自一般运动分割算法的弱学习信号来增强结果模型。实验表明，尽管只捕获了移动对象的一小部分，但该信号足以泛化，并完成对相应对象的移动和静态实例的分割。模型也可以扩展到一个新收集的具有街道驾驶场景的合成数据集。
1. 介绍（Introduction） ​	物体是感知的关键。人们理解世界不是以像素或是表面的形式，而是通过一个个物体及其集合。在计算机视觉领域，最近已经在物体认知领域取得了一些进展，但是这些已有的方法都需要代价昂贵的大规模手工标注，并且只能发现固定物体列表中的对象。因此此从图片数据中直接发现物体，即以跨领域通用的方式发现物体，依然是一个等待解决的问题。
​	让该问题显得尤其苦难的点在于，物体的概念是模糊的、上下文相关的。对一张汽车的照片，门和门把手既可以看成两个不同的物体，又可以看成一个整体。因此，超出受控场景时，试图基于外观，自动将对象从背景中分离出来的现有方法遭遇困难并不奇怪。
​	特别地，基于图的推理的经典方法往往会过少或过度分割对象。基于学习的方法使用结构化生成网络，对物体发现进行建模，通常在自动编码器的过程中利用迭代推理，虽然已经被证明了有不错的结果，但这些方法通常局限于在简单背景上具有彩色几何图形的玩具图像，在复杂现实场景中则完全失效。、
​	作者认为，虽然在静态的图片世界中物体定义的模糊性无法解决，但是当物体在动态的视频中，自然就与其他物体区分开来。具体地说，作者选择关注动态物体，并将其定义为能够在世界中独立移动的实体。独立的物体运动是一条强大的分组线索，已被证明可以驱动动物感知中的物体学习。
​	在计算机视觉领域，已经有一系列关于运动分割的工作，研究基于光流（Optical Flow）自动将运动的物体从背景中分离出来。这些方法也已经在无监督和弱监督机器学习算法中发现了大量应用。
​	在本文中，作者展示了运动分割如何作为引导去对实例分组，即使当它们处于静态。其方法基于 Locatello 提出的针对无监督物体发现的框架，作者也说明了如何将其进行修改以便可以从玩具图像泛化应用到现实视频。实际上是通过引入了时空记忆模块（Spatial-Temporal Memory Module），并简化了分组机制使得模型适应大分辨率和多物体场景，由此该结构可应用于任意长度的视频。
​	然后作者演示了基于独立对象运动的归纳偏差对初始表示的重要性，以及初始表示捕获对象的程度。特别地，作者展示了运动分割是如何引导注意力机制去发现静态物体的。至关重要的是，作者证明了不同质量的运动分割——即使是稀疏和有噪声的情况下——也足以使模型偏向于分割运动和静态实例。并且该方法仅需要视频用作训练，并在推断时分割静态图片中的物体。
2. 相关工作（Related Work） ​	在这项工作中，我们利用运动分割作为自底而上分组的学习信号来研究真实视频中的物体发现问题。下面，我们回顾了这些领域中最相关的工作。
2.1 物体发现（Object discovery） ​	传统的计算机视觉将物体发现视为感知分组任务，通过一些低级或中级的规律，比如颜色、方向、文本，来将场景分割为类似物体的区域。但是，由于对外形的依赖，这些方法都没有很好的解决物体定义的内在模糊性。
​	最近，基于学习的方法也被用于物体发现。在 Locatello 提出的 SlotAttention 框架中，首先用 CNN 对图片进行编码，然后进行迭代的注意力操作被用于将一套变量绑定到图片中，然后利用对这些变量的分开解码和联合，来重构图片。
​	在本文中，作者通过修改 SlotAttention 的结构，使其可以衡量带有多个物体的大尺度场景，并以归纳偏差（Inductive bias）的形式引入了运动分割，最终该结构可以应用于真实的视频。另外，模型只需要使用运动作为解析学习信号，并且训练好的模型可以同时分割静态和动态实例。
2.2 运动分割（Motion segmentation） ​	使用光流从背景中分离处物体，早期的方法跟踪流的单个像素，并根据共同命运原则对生成的轨迹进行聚类。这些方法虽然在运动分割的基准数据集上取得了不错的结果，但由于其本质是启发式的，所以不能很好地泛化。最近，基于学习的方法也被提出，Dave 的模型在 FlyingThings3D 数据集上训练，但是基于流提供的外型提取，可以泛化到真实视频。由于其良好的表现，本文作者也使用了 Dave 的方法。
3.方法论（Method） ​	首先在 3.1 中介绍本文研究的基础，即 SlotAttention 框架用于无监督物体发现。" />
  <meta name="author" content="MeanVery" />
  

  
  
  
  
  
  
  <link rel="preload stylesheet" as="style" href="https://meanvery.github.io/app.min.css" />

  
  <link rel="preload stylesheet" as="style" href="https://meanvery.github.io/an-old-hope.min.css" />
  <script
    defer
    src="https://meanvery.github.io/highlight.min.js"
    onload="hljs.initHighlightingOnLoad();"
  ></script>
  

  
  <link rel="preload" as="image" href="https://meanvery.github.io/theme.png" />

  
  <link rel="preload" as="image" href="https://meanvery.github.io/github.svg" />
  

  
  <link rel="icon" href="https://meanvery.github.io/favicon.ico" />
  <link rel="apple-touch-icon" href="https://meanvery.github.io/apple-touch-icon.png" />

  
  <meta name="generator" content="Hugo 0.101.0" />

  
  

  
  
  
  
  
  
  
  
  
  <meta property="og:title" content="分割-发现会动的物体" />
<meta property="og:description" content="Discovering Objects that Can Move" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://meanvery.github.io/post/2022-11-15-%E5%88%86%E5%89%B2-%E5%8F%91%E7%8E%B0%E4%BC%9A%E5%8A%A8%E7%9A%84%E7%89%A9%E4%BD%93/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2022-11-15T00:00:00+00:00" />
<meta property="article:modified_time" content="2022-11-15T00:00:00+00:00" />


  
  <meta itemprop="name" content="分割-发现会动的物体">
<meta itemprop="description" content="Discovering Objects that Can Move"><meta itemprop="datePublished" content="2022-11-15T00:00:00+00:00" />
<meta itemprop="dateModified" content="2022-11-15T00:00:00+00:00" />
<meta itemprop="wordCount" content="360">
<meta itemprop="keywords" content="" />
  
  <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="分割-发现会动的物体"/>
<meta name="twitter:description" content="Discovering Objects that Can Move"/>

  
  
</head>


  <body class="not-ready" data-menu="true">
    <header class="header">
  
  <p class="logo">
    <a class="site-name" href="https://meanvery.github.io/">Very的计算机学习</a><a class="btn-dark"></a>
  </p>
  

  <script>
    let bodyClx = document.body.classList;
    let btnDark = document.querySelector('.btn-dark');
    let sysDark = window.matchMedia('(prefers-color-scheme: dark)');
    let darkVal = localStorage.getItem('dark');

    let setDark = (isDark) => {
      bodyClx[isDark ? 'add' : 'remove']('dark');
      localStorage.setItem('dark', isDark ? 'yes' : 'no');
    };

    setDark(darkVal ? darkVal === 'yes' : sysDark.matches);
    requestAnimationFrame(() => bodyClx.remove('not-ready'));

    btnDark.addEventListener('click', () => setDark(!bodyClx.contains('dark')));
    sysDark.addEventListener('change', (event) => setDark(event.matches));
  </script>

  
  
  <nav class="menu">
    
    <a class="" href="/about/">关于</a>
    
  </nav>
  

  
  <nav class="social">
    
    <a
      class="github"
      style="--url: url(./github.svg)"
      href="https://github.com/MeanVery"
      target="_blank"
    ></a>
    
  </nav>
  
</header>


    <main class="main">

<article class="post-single">
  <header class="post-title">
    <p>
      
      <time>Nov 15, 2022</time>
      
      
      <span>MeanVery</span>
      
    </p>
    <h1>分割-发现会动的物体</h1>
  </header>
  <section class="post-content">

<!-- KaTeX -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>



<h2 id="discovering-objects-that-can-move">Discovering Objects that Can Move</h2>
<h4 id="摘要abstract">摘要（Abstract）</h4>
<p>​	本文研究了物体发现问题——在没有手动标签的情况下将对象从背景中分离出来。现有的方法利用外观的线索，如颜色、纹理和位置，将像素聚类成类似物体的区域。然而，仅依靠外观，这些方法无法在杂乱的场景中将物体与背景分离。这是一个基本的限制，因为对象的定义本质上是模糊的、上下文相关的。为了解决这种模糊性，本文选择关注动态物体——可以在世界中独立移动的实体。</p>
<p>​	作者将最新的基于自动编码器的框架，从玩具合成图像扩展到复杂现实场景的无监督物体发现。为此，简化了它们的结构，并使用来自一般运动分割算法的弱学习信号来增强结果模型。实验表明，尽管只捕获了移动对象的一小部分，但该信号足以泛化，并完成对相应对象的移动和静态实例的分割。模型也可以扩展到一个新收集的具有街道驾驶场景的合成数据集。</p>
<h4 id="1-介绍introduction">1. 介绍（Introduction）</h4>
<p>​	物体是感知的关键。人们理解世界不是以像素或是表面的形式，而是通过一个个物体及其集合。在计算机视觉领域，最近已经在物体认知领域取得了一些进展，但是这些已有的方法都需要代价昂贵的大规模手工标注，并且只能发现固定物体列表中的对象。因此此从图片数据中直接发现物体，即以跨领域通用的方式发现物体，依然是一个等待解决的问题。</p>
<p>​	让该问题显得尤其苦难的点在于，物体的概念是模糊的、上下文相关的。对一张汽车的照片，门和门把手既可以看成两个不同的物体，又可以看成一个整体。因此，超出受控场景时，试图基于外观，自动将对象从背景中分离出来的现有方法遭遇困难并不奇怪。</p>
<p>​	特别地，基于图的推理的经典方法往往会过少或过度分割对象。基于学习的方法使用结构化生成网络，对物体发现进行建模，通常在自动编码器的过程中利用迭代推理，虽然已经被证明了有不错的结果，但这些方法通常局限于在简单背景上具有彩色几何图形的玩具图像，在复杂现实场景中则完全失效。、</p>
<p>​	作者认为，虽然在静态的图片世界中物体定义的模糊性无法解决，但是当物体在动态的视频中，自然就与其他物体区分开来。具体地说，作者选择关注动态物体，并将其定义为能够在世界中独立移动的实体。独立的物体运动是一条强大的分组线索，已被证明可以驱动动物感知中的物体学习。</p>
<p>​	在计算机视觉领域，已经有一系列关于运动分割的工作，研究基于光流（Optical Flow）自动将运动的物体从背景中分离出来。这些方法也已经在无监督和弱监督机器学习算法中发现了大量应用。</p>
<p>​	在本文中，作者展示了运动分割如何作为引导去对实例分组，即使当它们处于静态。其方法基于 Locatello 提出的针对无监督物体发现的框架，作者也说明了如何将其进行修改以便可以从玩具图像泛化应用到现实视频。实际上是通过引入了时空记忆模块（Spatial-Temporal Memory Module），并简化了分组机制使得模型适应大分辨率和多物体场景，由此该结构可应用于任意长度的视频。</p>
<p>​	然后作者演示了基于独立对象运动的归纳偏差对初始表示的重要性，以及初始表示捕获对象的程度。特别地，作者展示了运动分割是如何引导注意力机制去发现静态物体的。至关重要的是，作者证明了不同质量的运动分割——即使是稀疏和有噪声的情况下——也足以使模型偏向于分割运动和静态实例。并且该方法仅需要视频用作训练，并在推断时分割静态图片中的物体。</p>
<h4 id="2-相关工作related-work">2. 相关工作（Related Work）</h4>
<p>​	在这项工作中，我们利用运动分割作为自底而上分组的学习信号来研究真实视频中的物体发现问题。下面，我们回顾了这些领域中最相关的工作。</p>
<h5 id="21-物体发现object-discovery">2.1 物体发现（Object discovery）</h5>
<p>​	传统的计算机视觉将物体发现视为感知分组任务，通过一些低级或中级的规律，比如颜色、方向、文本，来将场景分割为类似物体的区域。但是，由于对外形的依赖，这些方法都没有很好的解决物体定义的内在模糊性。</p>
<p>​	最近，基于学习的方法也被用于物体发现。在 Locatello 提出的 SlotAttention 框架中，首先用 CNN 对图片进行编码，然后进行迭代的注意力操作被用于将一套变量绑定到图片中，然后利用对这些变量的分开解码和联合，来重构图片。</p>
<p>​	在本文中，作者通过修改 SlotAttention 的结构，使其可以衡量带有多个物体的大尺度场景，并以归纳偏差（Inductive bias）的形式引入了运动分割，最终该结构可以应用于真实的视频。另外，模型只需要使用运动作为解析学习信号，并且训练好的模型可以同时分割静态和动态实例。</p>
<h5 id="22-运动分割motion-segmentation">2.2 运动分割（Motion segmentation）</h5>
<p>​	使用光流从背景中分离处物体，早期的方法跟踪流的单个像素，并根据共同命运原则对生成的轨迹进行聚类。这些方法虽然在运动分割的基准数据集上取得了不错的结果，但由于其本质是启发式的，所以不能很好地泛化。最近，基于学习的方法也被提出，Dave 的模型在 FlyingThings3D 数据集上训练，但是基于流提供的外型提取，可以泛化到真实视频。由于其良好的表现，本文作者也使用了 Dave 的方法。</p>
<h4 id="3方法论method">3.方法论（Method）</h4>
<p>​	首先在 3.1 中介绍本文研究的基础，即 SlotAttention 框架用于无监督物体发现。</p>
<p>​	然后在 3.2 中介绍如何将该结构用于多物体的真实视频。</p>
<p>​	最后在 3.3 中提出了融入独立运动先验的方法。</p>
<h5 id="31-背景background">3.1 背景（Background）</h5>
<p>​	给定一张图片 \(I\in \Reals^{H\times W\times 3}\) ，首先将它传入基于 CNN 的编码器，获取隐藏的特征表达：\(H=f_{enc}(I)\in \Reals^{H'\times W'\times 3}\) 。然后，经过注意力模块的处理，将 H 匹配到一系列长度固定为 K 的特征向量（被称为 slot），即 \(S\in \Reals^{K\times D_{slot}}\) 。每个 \(S_i\in S\) 被映射到 2 维网格中，然后经过基于 CNN 的编码器 \(O_i=f_{dec}(S_i)\in \Reals^{H\times W\times 4}\) 对其进行单独编码，其中第 4 个维度代表 alpha mask  \(A_i\) 。用 \(I'_i\) 表示 \(O_i\) 的前三个通道，完整的图片重构是通过 \(I'=\textstyle\sum_i A_i*I'_i\) 得到的，并用于在 MSE 损失下监督模型。</p>
<p>​	注意力权重的计算是用输入特征和 slot 做点积（dot product）得到的： \(W=\frac{1}{\sqrt{D}}k(H)\cdot q(S)\in \Reals^{N\times K}\) ，其中 k, q 是可以学习的线性变换， \(N=H'\times W'\) 。这些注意力权重被用于计算更新值 \(U=W^T v(H) \in \Reals^{K\times D}\) ，其中 W 是标准化后的权重，v 是另外一个线性变换。</p>
<p>​	和经典的 Transformer 不同的是，slots 是随机初始化的，推理过程是迭代的。具体地说，第 l 步中，slots 被更新为 \(S_l=update(S_{l-1},U_l)\)，其中更新函数是用门控循环单元（Gated recurrent unit, GRU）实现的。</p>
<p>​	这个方法背后的直觉在于，slots 是特征表达的关键，slots 的单独解码使得特征与空间相关区域（包括物体）绑定到一起。</p>
<h5 id="32-一个用于视频的物体发现框架a-framework-for-object-discovery-in-videos">3.2 一个用于视频的物体发现框架（A Framework for object discovery in videos）</h5>
<div>
    <center>
    <img src="/dotcm1.png"
         alt="框架示意图"
         style="zoom:60%"/>
    框架示意图
    </center>
</div>
<br>
<p>​	如图，一系列视频的帧 \(\{I^1, I^2,...,I^T\}\) 作为输入，然后经过 CNN 编码器编码，获得帧单独的特征表达 \(H^t=f_{enc} (I^t)\) 。这些单独的表达被一个卷积门控循环单元的时空记忆模块聚合，得到视频编码 \(H'^t=ConvGRU (R^{t-1}, H^t)\) ，其中 \(R^{t-1}\in \Reals^{H'\times W'\times D_{inp}}\) 是循环记忆状态。</p>
<p>​	接着，继续将\(H'^t\) 匹配到 slot，\(S^t\) 。作者选择只执行一个单独的注意力操作直接获得 slot 的状态\(S^t =W^{ t^T }v(H'^t)\) ，其中注意力矩阵 \(W^t\) 是通过前一帧中的的 slot 状态，\(S^{t-1}\) 计算出来的。而在第一帧中，则使用了一个预设的可学习的\(S^0\) 。</p>
<p>​	同时作者发现，只需将 slot 数量增加到最大预期物体数量，就足以使得模型可以泛化到不同的复杂场景。</p>
<p>​	最终的 slot 状态， \(S^t\) 需要用 CNN 解码器处理，得到重构的帧。作者提出了调换 slot 解码和 slot 重合并的顺序、。具体地说，首先将每个 slot 特征 \(S^t_i\in \Reals^D_{slot}\) 映射到特征图 \(F^t_i\in \Reals^{H'\times W'\times D_{slot}}\) ，并使用注意力掩码 \(W^t_{:,i}\) 作为 alpha mask  \(A^t_i\) 。然后作者构建了一个单独的输出特征图， \(F^t=\textstyle\sum_i A^t_i * F^t_i\) ，并通过 \(I'^t=f_{dec}(F_t)\in \Reals^{H\times W\times 3}\) 解码。</p>
<h5 id="33-融入独立的运动先验incorporating-independent-motion-priors">3.3 融入独立的运动先验（Incorporating independent motion priors）</h5>
<p>​	作者假定了一套稀疏、实例水平的运动分割掩码 \(M=\{M^1, M^2,..., M^T\}\)</p>
<p>，与每个视频一起被提供，\(M^t= \{m_1,m_2,...,m_{C^t}\}\) ，其中\(C^t\) 是在第 t 帧中被成功分割出来的运动着的物体，\(m_j\in \{0,1\}^{H'\times W'}\) 是一个二进制掩码。对每一帧，也可能根本没有物体，使得\(M_i=\text{\O\) 。这种假设也合理反映了，在给定的一些帧中有物体在运动，而另一些帧中所有物体都是静态的。</p>
<p>​	作者提出使用这些运动分割掩码来直接监督 slot 注意力图\(W^t\in \Reals^{N\times K}\) 。因此在每一帧中，需要将数量不定的运动分割\(C^t\) 匹配到固定数量为 \(K\) 的 slots 上。基于集合的监督，我们首先在预测和运动掩码之间找到最佳的由两个部分组成的匹配，然后优化特定于物体的分割损失。</p>
<p>​	具体来说，为了找到由两个部分组成的匹配，作者通过下列最低损失，寻找 \(K\) 个元素的排列：\(\hat{\sigma}=argmin_\sigma \textstyle{\sum}_{i=1}^K \mathcal{L}_{seg}(m^t_i, W^t_{:,\sigma(i)})\)</p>
<br>
<p>其中\(\mathcal{L}_{seg}(m^t_i, W^t_{:,\sigma(i)})\) 是运动掩码和 slot 特征图之间的分割损失。在实践中，作者使用贪婪匹配算法有效地逼近最优分配。</p>
<p>​	当 \(\hat{\sigma}\) 被计算出来之后，最后的运动监督物体被定义如下： \(\mathcal{L}_{motion}=\textstyle{\sum}_{i=1}^K 1_{m^t_i \neq 空集} \mathcal{L}_{seg}(m^t_i, W^t_{:,\sigma(i)})\) 。</p>
<p>​	也就是说，仅为已指定运动掩码的 slot 计算损失，其余 slot 不受约束，可以绑定到图像中的任何区域。运动分割掩码在拥挤的室外场景中仅适用于两个物体，并且它们和注意力图与遮罩最相似的 slot 相匹配。剩余的 slot 是不受限制的，但仍能够由图像重建目标驱动，捕获运动和静态对象以及背景。分割损失是二进制交叉熵（如下式）：</p>
\(\mathcal{L}_{seg}(m,W)=\textstyle{\sum^N_{j=1}}-m_j log(W_j)-(1-m_j)log(1-W_j)\)
<br>
<h5 id="34-损失函数和优化loss-function-and-optimization">3.4 损失函数和优化（Loss function and optimization）</h5>
<p>​	最终的目标函数如下：</p>
\(\mathcal{L}=\mathcal{L}_{recon}+\lambda_M\mathcal{L}_{motion}+\lambda_T\mathcal{L}_{temp}\)
<br>
<p>​	其中， \(\mathcal{L}_{recon}\) 是图像重建的 MSE 损失； \(\mathcal{L}_{temp}\) 是时间一致性正则项； \(\lambda_M, \lambda_T\) 分别是运动监督和时序一致项的权重，后者定义如下：</p>
\(\mathcal{L}_{temp}(S)=\textstyle\sum^{T-1}_{t=1}||I-softmax(S^t\cdot (S^{t+1} )^T)||\)
<br>
<p>​	其中， \(I\in \Reals^{K\times K}\) 是单位矩阵。上式鼓励了连续帧中 slot 的特征相似性，从而提高了 slot 绑定的时间一致性。该模型在长度为 T 的视频剪辑上进行训练，确保一个批次中至少有一半的剪辑具有非空的运动分割集 M。</p>
</section>

  
  

  
  
  
  <nav class="post-nav">
     
    <a class="next" href="https://meanvery.github.io/post/2022-11-14-%E5%88%86%E5%89%B2-%E4%BB%8E%E8%AF%AD%E4%B9%89%E5%88%B0%E5%AE%9E%E4%BE%8B/"><span>分割-从语义到实例</span><span>→</span></a>
    
  </nav>
  

  
  
</article>

</main>

    <footer class="footer">
  <p>&copy; 2022 <a href="https://meanvery.github.io/">Very的计算机学习</a></p>
  <p>Powered by <a href="https://gohugo.io/" rel="noopener" target="_blank">Hugo️️</a>️</p>
  <p>
    <a href="https://github.com/nanxiaobei/hugo-paper" rel="noopener" target="_blank">Paper 5.1</a>
  </p>
</footer>

  </body>
</html>
