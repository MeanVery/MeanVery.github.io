<!DOCTYPE html>













<html lang="en">
  <head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />

  
  <title>Seminar-20221010-Self Attention, Transformer - Very的计算机学习</title>

  
  
  <meta name="description" content="感谢陈露元同学、胡占斌同学的讲解！
Self Attention ​	首先是应用在语言处理领域，对词语进行位置编码，然后经过自注意力机制算法，就可以自动得到具备相关性的计算结果。这种相关性使得该结构可以掌握关联信息，捕捉到距离较长的词语之间的关系，使得翻译任务得到了更好的处理。
位置编码
在我看来，后续的计算其实是对编码的计算，那么就要保证编码尽可能全面地反应了对象的信息，同时还具备一些其他的功能，比如可以反映编码之间的关联。所以编码其实也是一个需要仔细斟酌的问题，实际上也经历了较多过程的演化。
用整型值标记位置
第一个位置标记 1，第二个位置标记 2……以此类推。
这种方式的缺点是，模型应用时遇到的序列可能比训练时所用的序列长，导致模型的泛化性能较差。其次，这种编码方式会让编码随着序列长度的增加越变越大，没有一个确定的界限。
用 [0,1] 范围标记位置
有多少个位置就在该区间内等距离地取多少个点的值。
这种方式下，当序列的长度不同时，位置之间的相对距离会变化，数值变化使得模型对位置之间的关系的理解出现偏差。
用二进制向量标记位置
缺点是编码出来的位置向量，其实在空间内是离散的，不同位置之间的变化不连续，会产生数学计算上的不方便。
用周期函数 sinx/cosx 标记位置
低位给予较小的周期，高位给予很大的周期，这样随着序列长度的增加，高位变动缓慢，低位变动迅速。同时可以使得位置向量的值是有界的且位于连续空间，模型在处理时就更容易泛化。并且还可以通过简单的计算，得到两个词向量的距离关系，有助于衡量词之间的关联。
自注意力机制
将得到的位置编码矩阵经过计算后，分别又得到 Q、K、V 矩阵，借助注意力公式： $$ Attention(Q,K,V)=softmax(QK^T/\sqrt d_k)V $$ 计算出每个词的注意力，相当于对初始的词向量，加入了整句话中与之有关的信息，重新优化了每个词的编码，使之内涵更加丰富。
Transformer1 Transformer 结构示意图​	encoder 对输入的内容进行位置编码，然后利用了多头注意力机制，即使用多个矩阵相乘，得到多组 Q、K、V 矩阵再进行后续的计算。Transformer 中也利用了残差的思想，将处理前的信息直接传递到经过多头注意力机制处理后的位置，通过加法融合在一起，再进行 LN，即层正则化，因为每一层代表了一个句子，层正则化才可以处理到一句话之类的信息，如果还是应用 BN 来处理，不同句子相对位置相同的词向量之间其实不具有有效的信息。然后把融合正则化完的结果进行前向传播，再次融合并正则化后作为一部分传递到 decoder。
​	decoder 实际上对训练时需要输出的数据，进行了位置编码并输入模型，按照同样的多头注意力机制算法进行计算，将输出的结果，结合 encoder 传递过来的部分，再进行一次注意力机制计算，就得到了输入和输出之间的注意力关系。最后经过一个利用了残差思想的前向传播网络，经过线性整合与 softmax 之后，得到输出结果的概率值。
https://arxiv.org/pdf/1706.03762.pdf&#160;&#x21a9;&#xfe0e;" />
  <meta name="author" content="MeanVery" />
  

  
  
  
  
  
  
  <link rel="preload stylesheet" as="style" href="https://meanvery.github.io/app.min.css" />

  
  <link rel="preload stylesheet" as="style" href="https://meanvery.github.io/an-old-hope.min.css" />
  <script
    defer
    src="https://meanvery.github.io/highlight.min.js"
    onload="hljs.initHighlightingOnLoad();"
  ></script>
  

  
  <link rel="preload" as="image" href="https://meanvery.github.io/theme.png" />

  
  <link rel="preload" as="image" href="https://meanvery.github.io/github.svg" />
  

  
  <link rel="icon" href="https://meanvery.github.io/favicon.ico" />
  <link rel="apple-touch-icon" href="https://meanvery.github.io/apple-touch-icon.png" />

  
  <meta name="generator" content="Hugo 0.101.0" />

  
  

  
  
  
  
  
  
  
  
  
  <meta property="og:title" content="Seminar-20221010-Self Attention, Transformer" />
<meta property="og:description" content="Seminar about Self Attention, Transformer" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://meanvery.github.io/post/seminar-20221010/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2022-10-11T00:00:00+00:00" />
<meta property="article:modified_time" content="2022-10-11T00:00:00+00:00" />


  
  <meta itemprop="name" content="Seminar-20221010-Self Attention, Transformer">
<meta itemprop="description" content="Seminar about Self Attention, Transformer"><meta itemprop="datePublished" content="2022-10-11T00:00:00+00:00" />
<meta itemprop="dateModified" content="2022-10-11T00:00:00+00:00" />
<meta itemprop="wordCount" content="53">
<meta itemprop="keywords" content="" />
  
  <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Seminar-20221010-Self Attention, Transformer"/>
<meta name="twitter:description" content="Seminar about Self Attention, Transformer"/>

  
  
</head>


  <body class="not-ready" data-menu="true">
    <header class="header">
  
  <p class="logo">
    <a class="site-name" href="https://meanvery.github.io/">Very的计算机学习</a><a class="btn-dark"></a>
  </p>
  

  <script>
    let bodyClx = document.body.classList;
    let btnDark = document.querySelector('.btn-dark');
    let sysDark = window.matchMedia('(prefers-color-scheme: dark)');
    let darkVal = localStorage.getItem('dark');

    let setDark = (isDark) => {
      bodyClx[isDark ? 'add' : 'remove']('dark');
      localStorage.setItem('dark', isDark ? 'yes' : 'no');
    };

    setDark(darkVal ? darkVal === 'yes' : sysDark.matches);
    requestAnimationFrame(() => bodyClx.remove('not-ready'));

    btnDark.addEventListener('click', () => setDark(!bodyClx.contains('dark')));
    sysDark.addEventListener('change', (event) => setDark(event.matches));
  </script>

  
  
  <nav class="menu">
    
    <a class="" href="/about/">关于</a>
    
  </nav>
  

  
  <nav class="social">
    
    <a
      class="github"
      style="--url: url(./github.svg)"
      href="https://github.com/MeanVery"
      target="_blank"
    ></a>
    
  </nav>
  
</header>


    <main class="main">

<article class="post-single">
  <header class="post-title">
    <p>
      
      <time>Oct 11, 2022</time>
      
      
      <span>MeanVery</span>
      
    </p>
    <h1>Seminar-20221010-Self Attention, Transformer</h1>
  </header>
  <section class="post-content">

<!-- KaTeX -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>



<p>感谢陈露元同学、胡占斌同学的讲解！</p>
<h2 id="self-attention">Self Attention</h2>
<p>​	首先是应用在语言处理领域，对词语进行位置编码，然后经过自注意力机制算法，就可以自动得到具备相关性的计算结果。这种相关性使得该结构可以掌握关联信息，捕捉到距离较长的词语之间的关系，使得翻译任务得到了更好的处理。</p>
<ul>
<li>
<p>位置编码</p>
<p>在我看来，后续的计算其实是对编码的计算，那么就要保证编码尽可能全面地反应了对象的信息，同时还具备一些其他的功能，比如可以反映编码之间的关联。所以编码其实也是一个需要仔细斟酌的问题，实际上也经历了较多过程的演化。</p>
<ol>
<li>
<p>用整型值标记位置</p>
<p>第一个位置标记 1，第二个位置标记 2……以此类推。</p>
<p>这种方式的缺点是，模型应用时遇到的序列可能比训练时所用的序列长，导致模型的泛化性能较差。其次，这种编码方式会让编码随着序列长度的增加越变越大，没有一个确定的界限。</p>
</li>
<li>
<p>用 [0,1] 范围标记位置</p>
<p>有多少个位置就在该区间内等距离地取多少个点的值。</p>
<p>这种方式下，当序列的长度不同时，位置之间的相对距离会变化，数值变化使得模型对位置之间的关系的理解出现偏差。</p>
</li>
<li>
<p>用二进制向量标记位置</p>
<p>缺点是编码出来的位置向量，其实在空间内是离散的，不同位置之间的变化不连续，会产生数学计算上的不方便。</p>
</li>
<li>
<p>用周期函数 sinx/cosx 标记位置</p>
<p>低位给予较小的周期，高位给予很大的周期，这样随着序列长度的增加，高位变动缓慢，低位变动迅速。同时可以使得位置向量的值是有界的且位于连续空间，模型在处理时就更容易泛化。并且还可以通过简单的计算，得到两个词向量的距离关系，有助于衡量词之间的关联。</p>
</li>
</ol>
</li>
<li>
<p>自注意力机制</p>
<p>将得到的位置编码矩阵经过计算后，分别又得到 Q、K、V 矩阵，借助注意力公式：
$$
Attention(Q,K,V)=softmax(QK^T/\sqrt d_k)V
$$
计算出每个词的注意力，相当于对初始的词向量，加入了整句话中与之有关的信息，重新优化了每个词的编码，使之内涵更加丰富。</p>
</li>
</ul>
<h2 id="transformer1cite">Transformer<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></cite></h2>
<div>
    <center>
    <img src="/transformer1.png"
         alt="Transformer 结构示意图"
         style="zoom:50%"/>
    Transformer 结构示意图
    </center>
</div>
<br>
<p>​	encoder 对输入的内容进行位置编码，然后利用了多头注意力机制，即使用多个矩阵相乘，得到多组 Q、K、V 矩阵再进行后续的计算。Transformer 中也利用了残差的思想，将处理前的信息直接传递到经过多头注意力机制处理后的位置，通过加法融合在一起，再进行 LN，即层正则化，因为每一层代表了一个句子，层正则化才可以处理到一句话之类的信息，如果还是应用 BN 来处理，不同句子相对位置相同的词向量之间其实不具有有效的信息。然后把融合正则化完的结果进行前向传播，再次融合并正则化后作为一部分传递到 decoder。</p>
<p>​	decoder 实际上对训练时需要输出的数据，进行了位置编码并输入模型，按照同样的多头注意力机制算法进行计算，将输出的结果，结合 encoder 传递过来的部分，再进行一次注意力机制计算，就得到了输入和输出之间的注意力关系。最后经过一个利用了残差思想的前向传播网络，经过线性整合与 softmax 之后，得到输出结果的概率值。</p>
<br>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p><a href="https://arxiv.org/pdf/1706.03762.pdf">https://arxiv.org/pdf/1706.03762.pdf</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
</section>

  
  

  
  
  
  <nav class="post-nav">
    
    <a class="prev" href="https://meanvery.github.io/post/seminar-20221021/"><span>←</span><span>Seminar-20221017-Bert, ViT</span></a>
     
    <a class="next" href="https://meanvery.github.io/post/seminar-20221007/"><span>Seminar-20221007-PAN, DANet</span><span>→</span></a>
    
  </nav>
  

  
  
</article>

</main>

    <footer class="footer">
  <p>&copy; 2022 <a href="https://meanvery.github.io/">Very的计算机学习</a></p>
  <p>Powered by <a href="https://gohugo.io/" rel="noopener" target="_blank">Hugo️️</a>️</p>
  <p>
    <a href="https://github.com/nanxiaobei/hugo-paper" rel="noopener" target="_blank">Paper 5.1</a>
  </p>
</footer>

  </body>
</html>
