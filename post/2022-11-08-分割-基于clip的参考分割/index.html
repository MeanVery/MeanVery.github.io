<!DOCTYPE html>













<html lang="en">
  <head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />

  
  <title>分割-基于 CLIP 的参考分割 - Very的计算机学习</title>

  
  
  <meta name="description" content="CRIS: CLIP-Driven Referring Image Segmentation ​	参考图像分割（Referring Image Segmentation，以下称其为“参考分割”）致力于研究分割出图片中符合语言描述的对象：给出词汇“人脸”，该研究就分割出图像中的人脸。
摘要（Abstract） ​	受到语言-图像对比预训练（Contrastive Language-Image Pretraining , CLIP）的启发，作者提出了 CRIS，设计了一个视觉语言解码器，将文本表示中细粒度的语义信息作用到每一个像素级别运算的激活上，增强了这两种模态的一致性。同时提出了文本到像素的对比学习方法，强迫文本特征和相关的像素级特征具有相似性，和不相关的像素级特征具有差异性。在无需任何后处理的情况下，CRIS 在三个基准数据集上的实验测试结果都显著优于之前的最佳结果。
1.介绍（Introduction） ​	与语义分割和实例分割不同的是，参考分割不局限于找出某个特定类别的对象，而是根据输入的语言表达找出对应的特定区域，显然如果这项技术发展成熟，可以用于交互图像编辑以及人机交互等领域。
​	但是由于语言文本和图像是不同的模态，各自具备不同的属性，因此很难使得文本特征与像素相匹配。不过得益于强大的深度学习，早期的方案是直接将文本特征和图像特征连接起来，使用合并得到的特征去生成分割的遮罩；随后，为了增强两种模态之间的交互，人们提出了一系列基于语言-图像注意力机制的方法来更好地学习跨模态的特征。
​	现在的方法则是利用外部给定知识加强学习效果，主要利用单模态预训练（预训练的图像或文本编码器），但是这就缺少了多模态间的关联信息。通过对大规模的无标注的数据进行语言监督，视觉语言预训练能够学习大量的多模态表示。CLIP 从 4 亿图片-文本对中学习到了图片级别的视觉概念，使得许多多模态任务取得了巨大的进展，包括图像-文本检索，视频-文本检索。但是，将 CLIP 直接应用到参考分割中，效果并不理想，如下图：
参考分割效果示意图​	​	因为图片级别预测和像素级别预测具有巨大的差异，前者关注图片的全局信息，而后者还需要学习细粒度即小区域的视觉表示。
​	在文章中，作者基于 CLIP 进行参考分割，尝试增强跨模态匹配的能力。考虑到参考分割的特点，作者提出了一个有效且灵活的框架——CLIP-Driven Referring Image Segmentation（CRIS），可以将 CLIP 得到的大量跨模态但是相关的知识用于完成文本-像素的匹配。
​	首先，作者提出了一个视觉语言解码器（visual-language decoder），一方面它可以通过自注意力机制，捕捉图片中距离较远但是相关的像素级特征；另一方面，它可以通过交叉注意力机制，自适应地将精细的文本特征融合到图片的像素级特征中。
​	其次，作者引入了文本-像素的对比学习，它可以在多模态空间中，将语言特征和相关的像素级特征匹配起来，同时分离不相关的像素级特征。基于这个方案中语言和像素级视觉特征的交互，模型可以明确学习到细粒度即小区域的视觉概念。
​	作者认为他们的研究主要有以下三个贡献：
提出了 CLIP-Driven Referring Image Segmentation（CRIS），将 CLIP 得到的知识用于匹配文本和像素； 创新地提出了”视觉语言解码器“、”文本-像素对比学习“来充分利用跨模态的知识； 在三个基准数据集上的实验结果显著优于之前的最佳水平。 2.相关工作（Related Work） 视觉-语言预训练（Vision-Language Pretraining） ​	利用语义监督下的大规模图片数据，人们提出了许多方法来通过文本表示学习到视觉表示。MIL-NCE 主要探索利用嘈杂的大规模的 Howto100M 教学视频，通过端到端的方式学习得到更好的视频编码器；SimVLM 利用大规模的弱监督来降低训练复杂性，采用单前缀语言以端到端的方式来建模目标。得益于从互联网中收集到了大规模的图像文本对，CLIP 在嵌入空间中对两个模态的表达式的匹配取得了引人瞩目的成功，它采用强大的语言模型和视觉特征编码器进行对比学习，捕捉显著的视觉特征实现零镜头（Zero-Shot）分类。最近一系列将 CLIP 模型所得知识用于下游任务（比如视频标题、视频-文本检索和图像生成）的工作都取得了卓越的成果。" />
  <meta name="author" content="MeanVery" />
  

  
  
  
  
  
  
  <link rel="preload stylesheet" as="style" href="https://meanvery.github.io/app.min.css" />

  
  <link rel="preload stylesheet" as="style" href="https://meanvery.github.io/an-old-hope.min.css" />
  <script
    defer
    src="https://meanvery.github.io/highlight.min.js"
    onload="hljs.initHighlightingOnLoad();"
  ></script>
  

  
  <link rel="preload" as="image" href="https://meanvery.github.io/theme.png" />

  
  <link rel="preload" as="image" href="https://meanvery.github.io/github.svg" />
  

  
  <link rel="icon" href="https://meanvery.github.io/favicon.ico" />
  <link rel="apple-touch-icon" href="https://meanvery.github.io/apple-touch-icon.png" />

  
  <meta name="generator" content="Hugo 0.101.0" />

  
  

  
  
  
  
  
  
  
  
  
  <meta property="og:title" content="分割-基于 CLIP 的参考分割" />
<meta property="og:description" content="CLIP-Driven Referring Image Segmentation" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://meanvery.github.io/post/2022-11-08-%E5%88%86%E5%89%B2-%E5%9F%BA%E4%BA%8Eclip%E7%9A%84%E5%8F%82%E8%80%83%E5%88%86%E5%89%B2/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2022-11-08T00:00:00+00:00" />
<meta property="article:modified_time" content="2022-11-08T00:00:00+00:00" />


  
  <meta itemprop="name" content="分割-基于 CLIP 的参考分割">
<meta itemprop="description" content="CLIP-Driven Referring Image Segmentation"><meta itemprop="datePublished" content="2022-11-08T00:00:00+00:00" />
<meta itemprop="dateModified" content="2022-11-08T00:00:00+00:00" />
<meta itemprop="wordCount" content="540">
<meta itemprop="keywords" content="" />
  
  <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="分割-基于 CLIP 的参考分割"/>
<meta name="twitter:description" content="CLIP-Driven Referring Image Segmentation"/>

  
  
</head>


  <body class="not-ready" data-menu="true">
    <header class="header">
  
  <p class="logo">
    <a class="site-name" href="https://meanvery.github.io/">Very的计算机学习</a><a class="btn-dark"></a>
  </p>
  

  <script>
    let bodyClx = document.body.classList;
    let btnDark = document.querySelector('.btn-dark');
    let sysDark = window.matchMedia('(prefers-color-scheme: dark)');
    let darkVal = localStorage.getItem('dark');

    let setDark = (isDark) => {
      bodyClx[isDark ? 'add' : 'remove']('dark');
      localStorage.setItem('dark', isDark ? 'yes' : 'no');
    };

    setDark(darkVal ? darkVal === 'yes' : sysDark.matches);
    requestAnimationFrame(() => bodyClx.remove('not-ready'));

    btnDark.addEventListener('click', () => setDark(!bodyClx.contains('dark')));
    sysDark.addEventListener('change', (event) => setDark(event.matches));
  </script>

  
  
  <nav class="menu">
    
    <a class="" href="/about/">关于</a>
    
  </nav>
  

  
  <nav class="social">
    
    <a
      class="github"
      style="--url: url(./github.svg)"
      href="https://github.com/MeanVery"
      target="_blank"
    ></a>
    
  </nav>
  
</header>


    <main class="main">

<article class="post-single">
  <header class="post-title">
    <p>
      
      <time>Nov 8, 2022</time>
      
      
      <span>MeanVery</span>
      
    </p>
    <h1>分割-基于 CLIP 的参考分割</h1>
  </header>
  <section class="post-content">

<!-- KaTeX -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>



<h2 id="cris-clip-driven-referring-image-segmentation">CRIS: CLIP-Driven Referring Image Segmentation</h2>
<p>​	参考图像分割（Referring Image Segmentation，以下称其为“参考分割”）致力于研究分割出图片中符合语言描述的对象：给出词汇“人脸”，该研究就分割出图像中的人脸。</p>
<h4 id="摘要abstract">摘要（Abstract）</h4>
<p>​	受到语言-图像对比预训练（Contrastive Language-Image Pretraining , CLIP）的启发，作者提出了 CRIS，设计了一个视觉语言解码器，将文本表示中细粒度的语义信息作用到每一个像素级别运算的激活上，增强了这两种模态的一致性。同时提出了文本到像素的对比学习方法，强迫文本特征和相关的像素级特征具有相似性，和不相关的像素级特征具有差异性。在无需任何后处理的情况下，CRIS 在三个基准数据集上的实验测试结果都显著优于之前的最佳结果。</p>
<h4 id="1介绍introduction">1.介绍（Introduction）</h4>
<p>​	与语义分割和实例分割不同的是，参考分割不局限于找出某个特定类别的对象，而是根据输入的语言表达找出对应的特定区域，显然如果这项技术发展成熟，可以用于交互图像编辑以及人机交互等领域。</p>
<p>​	但是由于语言文本和图像是不同的模态，各自具备不同的属性，因此很难使得文本特征与像素相匹配。不过得益于强大的深度学习，早期的方案是直接将文本特征和图像特征连接起来，使用合并得到的特征去生成分割的遮罩；随后，为了增强两种模态之间的交互，人们提出了一系列基于语言-图像注意力机制的方法来更好地学习跨模态的特征。</p>
<p>​	现在的方法则是利用外部给定知识加强学习效果，主要利用单模态预训练（预训练的图像或文本编码器），但是这就缺少了多模态间的关联信息。通过对大规模的无标注的数据进行语言监督，视觉语言预训练能够学习大量的多模态表示。CLIP 从 4 亿图片-文本对中学习到了图片级别的视觉概念，使得许多多模态任务取得了巨大的进展，包括图像-文本检索，视频-文本检索。但是，将 CLIP 直接应用到参考分割中，效果并不理想，如下图：</p>
<div>
    <center>
    <img src="/cris1.png"
         alt="参考分割效果示意图"
         style="zoom:50%"/>
    参考分割效果示意图
    </center>
</div>
<p>​	<br></p>
<p>​	因为图片级别预测和像素级别预测具有巨大的差异，前者关注图片的全局信息，而后者还需要学习细粒度即小区域的视觉表示。</p>
<p>​	在文章中，作者基于 CLIP 进行参考分割，尝试增强跨模态匹配的能力。考虑到参考分割的特点，作者提出了一个有效且灵活的框架——CLIP-Driven Referring Image Segmentation（CRIS），可以将 CLIP 得到的大量跨模态但是相关的知识用于完成文本-像素的匹配。</p>
<p>​	首先，作者提出了一个视觉语言解码器（visual-language decoder），一方面它可以通过自注意力机制，捕捉图片中距离较远但是相关的像素级特征；另一方面，它可以通过交叉注意力机制，自适应地将精细的文本特征融合到图片的像素级特征中。</p>
<p>​	其次，作者引入了文本-像素的对比学习，它可以在多模态空间中，将语言特征和相关的像素级特征匹配起来，同时分离不相关的像素级特征。基于这个方案中语言和像素级视觉特征的交互，模型可以明确学习到细粒度即小区域的视觉概念。</p>
<p>​	作者认为他们的研究主要有以下三个贡献：</p>
<ul>
<li>提出了 CLIP-Driven Referring Image Segmentation（CRIS），将 CLIP 得到的知识用于匹配文本和像素；</li>
<li>创新地提出了”视觉语言解码器“、”文本-像素对比学习“来充分利用跨模态的知识；</li>
<li>在三个基准数据集上的实验结果显著优于之前的最佳水平。</li>
</ul>
<h4 id="2相关工作related-work">2.相关工作（Related Work）</h4>
<h5 id="视觉-语言预训练vision-language-pretraining">视觉-语言预训练（Vision-Language Pretraining）</h5>
<p>​	利用语义监督下的大规模图片数据，人们提出了许多方法来通过文本表示学习到视觉表示。MIL-NCE 主要探索利用嘈杂的大规模的 Howto100M 教学视频，通过端到端的方式学习得到更好的视频编码器；SimVLM 利用大规模的弱监督来降低训练复杂性，采用单前缀语言以端到端的方式来建模目标。得益于从互联网中收集到了大规模的图像文本对，CLIP 在嵌入空间中对两个模态的表达式的匹配取得了引人瞩目的成功，它采用强大的语言模型和视觉特征编码器进行对比学习，捕捉显著的视觉特征实现零镜头（Zero-Shot）分类。最近一系列将 CLIP 模型所得知识用于下游任务（比如视频标题、视频-文本检索和图像生成）的工作都取得了卓越的成果。</p>
<p>​	与这些工作不同的是，作者将 CLIP 得到的图片级别的视觉概念用于参考分割，实现对跨模态相关信息的利用。</p>
<h5 id="对比学习contrastive-learning">对比学习（Contrastive Learning）</h5>
<p>​	最近，VADeR 和 DenseCL 探索像素级对比学习，以填补自监督表示学习和密集预测任务之间的空白。另外，CLIP 提供了一个有前景的选择，即使用跨模态的对比损失，从收集到的大规模图片-文本对中直接学习可迁移的视觉概念。</p>
<p>​	在文章中，作者提出的 CRIS 就是借助 CLIP 的知识，使用端到端的方式进行参考分割。</p>
<h5 id="参考分割referring-image-segmentation">参考分割（Referring Image Segmentation）</h5>
<p>​	参考分割，即在理解给定的自然语言表达的基础上，在一张图片中分割出目标区域。</p>
<p>​	早期的工作主要是利用 CNN 和 LSTM，分开提取视觉和语言特征，然后直接将两个模态的特征连接起来，再利用 FCN 获得最终的分割结果。另外，MCN 设计了一个框架取得了不错的效果，它通过学习来同时优化对参考文本的理解和对图像的分割。</p>
<p>​	注意力机制在提取和语义相关的视觉特征上表现强大。LTS 引入位置优先（Position Prior），设计了一个通路将参考分割任务解耦为“定位再分割”。EFNet 设计了一种共同注意机制（Co-attention Mechanism），使用语言逐步细化多模态特征，来增强跨模态信息表示的一致性。VLT 使用 Transformer 构建具有编码器-解码器（Encoder-Decoder）注意力机制的网络来增强全局上下文信息。</p>
<p>​	而作者的目标是利用 CLIP 已经学到的知识，提高多模态信息的兼容性、跨模态匹配的能力。</p>
<h4 id="3方法论methodology">3.方法论（Methodology）</h4>
<div>
    <center>
    <img src="/cris2.png"
         alt="CRIS 框架示意图"
         style="zoom:50%"/>
    CRIS 框架示意图
    </center>
</div>
<br>
<p>​	首先，作者使用了 ResNet 和 Transformer 来分开提取图片和文本特征，之后将它们进一步融合以获取简单的多模态特征。其次，将得到的多模态特征和文本特征一起输入视觉-语言解码器，将从文本表示中得到的细粒度语义特征，传播到像素级的视觉激活（Visual Activation）中。最后，作者使用两个映射来生成最终的预测掩模，并采用文本到像素的对比损失，来明确地匹配文本特征与相关像素级视觉特征。</p>
<h5 id="31图片-文本特征提取image--text-feature-extraction">3.1图片-文本特征提取（Image &amp; Text Feature Extraction）</h5>
<p>​	CRIS 的输入包含了一张图片 \(I\) ，和一句参考文本表达 \(T\) 。</p>
<ul>
<li>
<p>图片编码器（Image Encoder）</p>
<p>输入图片 \(I \in \Reals^{H \times W \times 3} \) ，作者使用了 ResNet 中第 2 到 4 层的多尺度特征图，分别定义为 \(F_{v2} \in \Reals^{H/8 \times W/8 \times C_2} \) 、 \(F_{v3} \in \Reals^{H/16 \times W/16 \times C_3} \) 和 \(F_{v4} \in \Reals^{H/32 \times W/32 \times C_4} \) 。</p>
</li>
<li>
<p>文本编码器（Text Encoder）</p>
<p>输入表达 \(T \in \Reals^L \) ，作者使用了经过调整的 Transformer 来提取文本特征 \(F_t \in \Reals^{L \times C} \) 。Transformer 对文本的小写字节对编码（Byte Pair Encoding, BPE）表示进行操作，字词大小为49，152；文本序列用 [SOS] 和 [EOS] 标记括起来。[EOS] 标记处的 Transformer 最高层的激活被进一步转换为全局文本表示 \(F_s \in \Reals^{C'} \) 。</p>
</li>
<li>
<p>跨模态枢纽（Cross-modal Neck）</p>
<p>基于多尺度的视觉特征和全局文本表达 \(F_s\) ，作者通过融合 \(F_s\) 和 \(F_{v4}\) 得到简单的多模态特征 \(F_{m4} \in \Reals^{H/16 \times W/16 \times C} \) 如下：
$$
F_{m4}=Up(\sigma(F_{v4} W_{v4})\cdot \sigma(F_s W_s))
$$
其中， \(Up(\cdot)\) 表示 2 倍上采样； \(\cdot \) 表示逐元素相乘； \(\sigma \) 表示 ReLU 函数； \(W_{v4},W_s\) 是两个可学习的变换矩阵，将视觉特征和文本特征变换成同一维度。之后得到多模态特征 \(F_{m3}\) 如下：
$$
F_{m3}=[\sigma(F_{m4} W_{m4}), \sigma(F_{v3} W_{v3})]
$$</p>
<p>得到 \(F_{m2}\) 如下：</p>
\(F_{m2}=[\sigma(F_{m3} W_{m3}), \sigma(F'_{v2} W_{v2})],F'_{v2}=Avg(F_{v2})\)
<p>其中， \(Avg(\cdot)\) 表示核大小为 \(2 \times 2\) ，步长为 2 的平均池化； \([ , ]\) 表示连接操作。</p>
<p>随后，作者将三个多模态特征相连，并使用 \(1 \times 1\) 卷积来融合它们，公式如下：
$$
F_m=Conv([F_{m2},F_{m3},F_{m4}])
$$
其中， \(F_m \in \Reals^{H/16 \times W/16 \times C} \) 。</p>
<p>随后，作者利用 \(3 \times 3\) 卷积将一个二维的空间坐标特征 \(F_{coord} \in \Reals^{H/16 \times W/16 \times 2} \) 与 \(F_m \) 融合，得到的视觉特征 \(F_v \in \Reals^{H/16 \times W/16 \times C} \) 如下：
$$
F_v=Conv([F_m,F_{coord}])
$$
接着，作者将空间中的 \(F_v \) 展开成一个序列，得到视觉特征 \(F_v \in \Reals^{N \times C} \) , \(N=H/16 \times W/16 \) 。</p>
</li>
</ul>
<h5 id="32视觉语言解码器vision-language-decoder">3.2视觉语言解码器（Vision-Language Decoder）</h5>
<p>​	作者设计这个解码器用于自适应地，将源于文本特征的细粒度的语义信息传播到视觉特征当中。</p>
<p>​	该解码器的输入是文本特征 \(F_t \in \Reals^{L \times C} \) 和像素级视觉特征 \(F_v \in \Reals^{N \times C} \) 。为了捕获位置信息，将固定正弦空间位置编码分别添加到 \(F_v \) 和 \(F_t \) 。n层（n 在实验中被探讨）的视觉语言解码器用于生成一个进化的多模态特征序列 \(F_c \in \Reals^{N \times C} \)</p>
<p>​	遵循 Transformer 的标准结构，解码器中的每一层都包含了一个多头自注意机制层、一个多头交叉注意机制层、一个前馈神经网络。</p>
<p>​	 \(F_v \) 首先被输入多头自注意力机制层来捕捉全局上下文信息：
$$
F&rsquo;_v=MHSA(LN(F_v))+F_v
$$
​	其中， \(F'_v \) 是更新后的特征； \(MHSA(\cdot) \) 和 \(LN(\cdot) \) 表示多头自注意力机制和层标准化（Layer Normalization）。</p>
<p>​	之后，多头交叉注意力机制层将细粒度的语义信息传播到更新后的视觉特征中。为了获得多模态的特征 \(F_c \) ，交叉注意力机制的输出还将经过一个两层的感知机（MLP）运算，以及层标准化和残差连接，公式如下：
$$
F&rsquo;_c=MHCA(LN(F&rsquo;_v),F_t)+F&rsquo;_v
$$</p>
<p>$$
F_c=MLP(LN(F&rsquo;_c))+F&rsquo;_c
$$</p>
<p>​	其中， \(MHCA(\cdot) \) 表示多头交叉注意力机制层； \(F'_c \) 表示中间过渡特征；最终更新得到的特征 \(F_c \) 被用于最后的分割任务。</p>
<h5 id="33文本到像素的对比学习text-to-pixel-contrastive-learning">3.3文本到像素的对比学习（Text-to-Pixel Contrastive Learning）</h5>
<p>​	尽管 CLIP 通过匹配文本表达和图片级别表达，学习得到了大量的图片级别的视觉概念，它的这些知识对于解决参考分割问题来说是次优的——因为缺少更细粒度的视觉概念。</p>
<p>​	为了解决这个问题，作者设计了一个文本到像素的对比损失，可以明确地将文本特征匹配到与之相关的像素级视觉特征。如 CRIS 的框架示意图所示，图片和文本映射被用于对 \(F_c \) 和 \(F_s \) 进行如下变换：
$$
z_v=F&rsquo;_cW_v+b_v,F&rsquo;_c=Up(F_c)
$$</p>
<p>$$
z_t=F_sW_t+b_t
$$</p>
<p>​	其中， \(z_t \in \Reals^D \)， \(z_v \in \Reals^{N \times D} \) 。N的尺寸为 \(H/4 \times W/4\) ， \(Up \) 表示 4 倍上采样， \(W_v,W_t\) 是两个可学习的变换矩阵，将 \(F_c \) 和 \(F_s \) 变换成同一维度 \(D \) ， \(b_v,b_t\) 是两个可学习的偏置量。</p>
<p>​	基于得到的文本特征 \(z_t\) 和一系列像素级特征 \(z_v\) ，作者使用了一个对比损失函数来优化两个模态之间的关系： \(z_t\) 会趋于变得和与之相关的 \(z_v\) 相似，而和与之不相关的 \(z_v\) 更加迥异。</p>
<p>​	在点积（dot product）表示的相似性下，文本到像素的对比损失公式化如下：</p>
<p>​	当像素属于 \(\mathcal{P}\) 时：</p>
\(L^i_{con} (z_t,z_v^i)=-log \sigma(z_t \cdot z^i_v)\)
<p>​	当像素属于 \(\mathcal{N}\) 时：</p>
\(L^i_{con} (z_t,z_v^i)=-log(1-\sigma(z_t \cdot z^i_v))\)
<p>​	 综合起来看可以得到整体公式如下：
$$
L_{con}(z_t, z_v)=\frac{1}{|\mathcal{P}\cup\mathcal{N}|}\sum_{\begin{subarray}{l}i\in\mathcal{P}\cup\mathcal{N}
\end{subarray}}L^i_{con}(z_t, z^i_v)
$$
​	其中，\(\mathcal{P},\mathcal{N}\) 表示真实情况（Ground Truth）的 1（是目标对象）和 0（不是目标对象）， \(\sigma\) 表示 sigmoid 函数。最后，为了获得最终的分割结果，作者将 \(\sigma(z_t \cdot z_v)\) 转换成 \(H/4 \times W/4\) ，再将其上采样恢复到原图的尺寸。</p>
<br>
</section>

  
  

  
  
  
  <nav class="post-nav">
    
    <a class="prev" href="https://meanvery.github.io/post/2022-11-09-%E5%88%86%E5%89%B2-%E5%85%A8%E6%99%AF%E5%9B%BE%E5%83%8F%E7%9A%84%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/"><span>←</span><span>分割-全景图像的语义分割</span></a>
     
    <a class="next" href="https://meanvery.github.io/post/seminar-20221021/"><span>Seminar-20221017-BERT, ViT</span><span>→</span></a>
    
  </nav>
  

  
  
</article>

</main>

    <footer class="footer">
  <p>&copy; 2023 <a href="https://meanvery.github.io/">Very的计算机学习</a></p>
  <p>Powered by <a href="https://gohugo.io/" rel="noopener" target="_blank">Hugo️️</a>️</p>
  <p>
    <a href="https://github.com/nanxiaobei/hugo-paper" rel="noopener" target="_blank">Paper 5.1</a>
  </p>
</footer>

  </body>
</html>
