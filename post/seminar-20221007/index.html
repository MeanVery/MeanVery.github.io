<!DOCTYPE html>













<html lang="en">
  <head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />

  
  <title>Seminar-20221007-PAN, DANet - Very的计算机学习</title>

  
  
  <meta name="description" content="感谢周羿旭老师的讲解！
PAN1 ​	场景中的文字检测一直是人们感兴趣的任务，近年来也取得了不错的发展，但是依然存在两个较大的困难阻碍了该技术在实际生活中的运用：一是速度和精度不平衡；二是对复杂形状文字的建模很困难。
PAN思路示意图​	Pixel Aggregation Network （PAN）的思路是，用分割网络预测文字所在的区域，并得到“核参数”以及相似向量，然后在预测的核中重建完整的文字实例，最后可以预测分割任意形状的文字实例。PAN 整体结构示意图​	输入图片后，利用一个轻量化的网络结构对原始图片进行卷积运算，得到不同尺度的特征图，然后将不同尺度的特征图的通道数统一减小为某个值，接着通过特征金字塔强化单元（Feature Pyramid Enhancement Module）计算，再通过特征融合单元（Feature Fusion Module）进行特征融合，得到最终的特征图，再从该特征图中得到想要的分割区域、核参数、相似向量。
DANet2 ​	Dual Attention Network（DANet）双重注意力网络，特点是自适应地集成局部特征和全局依赖，在传统的扩张 FCN 上附加了两种类型的注意力模块，分别模拟空间和通道维度中的语义依赖性。
位置注意力模块
通过所处位置处的特征加权来选择性地聚合每个位置的特征，无论距离如何，类似的特征都会被关联起来。
通道注意力模块
整合所有通道映射间的相关特征来选择性地强调存在相互依赖的通道映射。
​	将以上两个模块的输出相加来改进特征的表示，得到了更精确的分割结果。
DANet 整体结构示意图​	reshape 表示将矩阵展开成一维向量，transpose 表示进行转置操作。其余过程则完全符合注意力机制的运算过程。最后将两个模块的结果融合成最终的特征图，再计算得到分割结果。
https://arxiv.org/pdf/1908.05900.pdf&#160;&#x21a9;&#xfe0e;
https://arxiv.org/pdf/1809.02983.pdf&#160;&#x21a9;&#xfe0e;" />
  <meta name="author" content="MeanVery" />
  

  
  
  
  
  
  
  <link rel="preload stylesheet" as="style" href="https://meanvery.github.io/app.min.css" />

  
  <link rel="preload stylesheet" as="style" href="https://meanvery.github.io/an-old-hope.min.css" />
  <script
    defer
    src="https://meanvery.github.io/highlight.min.js"
    onload="hljs.initHighlightingOnLoad();"
  ></script>
  

  
  <link rel="preload" as="image" href="https://meanvery.github.io/theme.png" />

  
  <link rel="preload" as="image" href="https://meanvery.github.io/github.svg" />
  

  
  <link rel="icon" href="https://meanvery.github.io/favicon.ico" />
  <link rel="apple-touch-icon" href="https://meanvery.github.io/apple-touch-icon.png" />

  
  <meta name="generator" content="Hugo 0.101.0" />

  
  

  
  
  
  
  
  
  
  
  
  <meta property="og:title" content="Seminar-20221007-PAN, DANet" />
<meta property="og:description" content="Seminar about PAN, DANet" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://meanvery.github.io/post/seminar-20221007/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2022-10-08T00:00:00+00:00" />
<meta property="article:modified_time" content="2022-10-08T00:00:00+00:00" />


  
  <meta itemprop="name" content="Seminar-20221007-PAN, DANet">
<meta itemprop="description" content="Seminar about PAN, DANet"><meta itemprop="datePublished" content="2022-10-08T00:00:00+00:00" />
<meta itemprop="dateModified" content="2022-10-08T00:00:00+00:00" />
<meta itemprop="wordCount" content="40">
<meta itemprop="keywords" content="" />
  
  <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Seminar-20221007-PAN, DANet"/>
<meta name="twitter:description" content="Seminar about PAN, DANet"/>

  
  
</head>


  <body class="not-ready" data-menu="true">
    <header class="header">
  
  <p class="logo">
    <a class="site-name" href="https://meanvery.github.io/">Very的计算机学习</a><a class="btn-dark"></a>
  </p>
  

  <script>
    let bodyClx = document.body.classList;
    let btnDark = document.querySelector('.btn-dark');
    let sysDark = window.matchMedia('(prefers-color-scheme: dark)');
    let darkVal = localStorage.getItem('dark');

    let setDark = (isDark) => {
      bodyClx[isDark ? 'add' : 'remove']('dark');
      localStorage.setItem('dark', isDark ? 'yes' : 'no');
    };

    setDark(darkVal ? darkVal === 'yes' : sysDark.matches);
    requestAnimationFrame(() => bodyClx.remove('not-ready'));

    btnDark.addEventListener('click', () => setDark(!bodyClx.contains('dark')));
    sysDark.addEventListener('change', (event) => setDark(event.matches));
  </script>

  
  
  <nav class="menu">
    
    <a class="" href="/about/">关于</a>
    
  </nav>
  

  
  <nav class="social">
    
    <a
      class="github"
      style="--url: url(./github.svg)"
      href="https://github.com/MeanVery"
      target="_blank"
    ></a>
    
  </nav>
  
</header>


    <main class="main">

<article class="post-single">
  <header class="post-title">
    <p>
      
      <time>Oct 8, 2022</time>
      
      
      <span>MeanVery</span>
      
    </p>
    <h1>Seminar-20221007-PAN, DANet</h1>
  </header>
  <section class="post-content">

<!-- KaTeX -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>



<p>感谢周羿旭老师的讲解！</p>
<h2 id="pan1cite">PAN<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></cite></h2>
<p>​	场景中的文字检测一直是人们感兴趣的任务，近年来也取得了不错的发展，但是依然存在两个较大的困难阻碍了该技术在实际生活中的运用：一是速度和精度不平衡；二是对复杂形状文字的建模很困难。</p>
<div>
    <center>
    <img src="/pan1.png"
         alt="PAN 思路示意图"
         style="zoom:50%"/>
    PAN思路示意图
    </center>
</div>
<br>
​	Pixel Aggregation Network （PAN）的思路是，用分割网络预测文字所在的区域，并得到“核参数”以及相似向量，然后在预测的核中重建完整的文字实例，最后可以预测分割任意形状的文字实例。
<div>
    <center>
    <img src="/pan2.png"
         alt="PAN 整体结构示意图"
         style="zoom:50%"/>
    PAN 整体结构示意图
    </center>
</div>
<br>
<p>​	输入图片后，利用一个轻量化的网络结构对原始图片进行卷积运算，得到不同尺度的特征图，然后将不同尺度的特征图的通道数统一减小为某个值，接着通过特征金字塔强化单元（Feature Pyramid Enhancement Module）计算，再通过特征融合单元（Feature Fusion Module）进行特征融合，得到最终的特征图，再从该特征图中得到想要的分割区域、核参数、相似向量。</p>
<h2 id="danet2cite">DANet<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup></cite></h2>
<p>​	Dual Attention Network（DANet）双重注意力网络，特点是自适应地集成局部特征和全局依赖，在传统的扩张 FCN 上附加了两种类型的注意力模块，分别模拟空间和通道维度中的语义依赖性。</p>
<ul>
<li>
<p>位置注意力模块</p>
<p>通过所处位置处的特征加权来选择性地聚合每个位置的特征，无论距离如何，类似的特征都会被关联起来。</p>
</li>
<li>
<p>通道注意力模块</p>
<p>整合所有通道映射间的相关特征来选择性地强调存在相互依赖的通道映射。</p>
</li>
</ul>
<p>​	将以上两个模块的输出相加来改进特征的表示，得到了更精确的分割结果。</p>
<div>
    <center>
    <img src="/dan1.png"
         alt="DANet 整体结构示意图"
         style="zoom:50%"/>
    DANet 整体结构示意图
    </center>
</div>
<br>
<p>​	reshape 表示将矩阵展开成一维向量，transpose 表示进行转置操作。其余过程则完全符合注意力机制的运算过程。最后将两个模块的结果融合成最终的特征图，再计算得到分割结果。</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p><a href="https://arxiv.org/pdf/1908.05900.pdf">https://arxiv.org/pdf/1908.05900.pdf</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p><a href="https://arxiv.org/pdf/1809.02983.pdf">https://arxiv.org/pdf/1809.02983.pdf</a>&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
</section>

  
  

  
  
  
  <nav class="post-nav">
    
    <a class="prev" href="https://meanvery.github.io/post/seminar-20221010/"><span>←</span><span>Seminar-20221010-Self Attention, Transformer</span></a>
     
    <a class="next" href="https://meanvery.github.io/post/seminar-20221003/"><span>Seminar-20221003-RefineNet, HRNet</span><span>→</span></a>
    
  </nav>
  

  
  
</article>

</main>

    <footer class="footer">
  <p>&copy; 2022 <a href="https://meanvery.github.io/">Very的计算机学习</a></p>
  <p>Powered by <a href="https://gohugo.io/" rel="noopener" target="_blank">Hugo️️</a>️</p>
  <p>
    <a href="https://github.com/nanxiaobei/hugo-paper" rel="noopener" target="_blank">Paper 5.1</a>
  </p>
</footer>

  </body>
</html>
