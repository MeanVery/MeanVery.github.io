<!DOCTYPE html>













<html lang="en">
  <head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />

  
  <title>An Overview of Computer Vision - Very的计算机学习</title>

  
  
  <meta name="description" content="Computer Vision1 Image Classification(图像分类)：将图片按内容分到某一类别 Object Detection(物体检测)：检测出图片中的目标对象 Segmentation(图像分割)：将图片按内容分割 Semantic Segmentation(语义分割)：不同物体分成不同的部分 Instance Segmentation(实例分割)：同一物体要分为不同个体 Image Generation(图像生成) 1 Neural Network 1.1 Forward Propogation(前向传播) The input data should be fed in the forward direction only. 神经元通过加权和与激活函数决定是否进一步传递数据。 常见激活函数：Sigmoid, ReLU, Softmax, Hyperbolic Tangent
1.2 Back Propogation(反向传播) It is the practice of fine-tuning the weights of a neural net based on the error rate obtained in the previous epoch.
1.3 Several Loss Function(损失函数) Mean Absolute Error(平均绝对误差) Mean Square Error(均方误差) Hinge Loss(铰链损失) Cross Entropy Loss(交叉熵损失) 1." />
  <meta name="author" content="MeanVery" />
  

  
  
  
  
  
  
  <link rel="preload stylesheet" as="style" href="https://meanvery.github.io/app.min.css" />

  
  <link rel="preload stylesheet" as="style" href="https://meanvery.github.io/an-old-hope.min.css" />
  <script
    defer
    src="https://meanvery.github.io/highlight.min.js"
    onload="hljs.initHighlightingOnLoad();"
  ></script>
  

  
  <link rel="preload" as="image" href="https://meanvery.github.io/theme.png" />

  
  <link rel="preload" as="image" href="https://meanvery.github.io/github.svg" />
  

  
  <link rel="icon" href="https://meanvery.github.io/favicon.ico" />
  <link rel="apple-touch-icon" href="https://meanvery.github.io/apple-touch-icon.png" />

  
  <meta name="generator" content="Hugo 0.101.0" />

  
  

  
  
  
  
  
  
  
  
  
  <meta property="og:title" content="An Overview of Computer Vision" />
<meta property="og:description" content="Computer Vision1 Image Classification(图像分类)：将图片按内容分到某一类别 Object Detection(物体检测)：检测出图片中的目标对象 Segmentation(图像分割)：将图片按内容分割 Semantic Segmentation(语义分割)：不同物体分成不同的部分 Instance Segmentation(实例分割)：同一物体要分为不同个体 Image Generation(图像生成) 1 Neural Network 1.1 Forward Propogation(前向传播) The input data should be fed in the forward direction only. 神经元通过加权和与激活函数决定是否进一步传递数据。 常见激活函数：Sigmoid, ReLU, Softmax, Hyperbolic Tangent
1.2 Back Propogation(反向传播) It is the practice of fine-tuning the weights of a neural net based on the error rate obtained in the previous epoch.
1.3 Several Loss Function(损失函数) Mean Absolute Error(平均绝对误差) Mean Square Error(均方误差) Hinge Loss(铰链损失) Cross Entropy Loss(交叉熵损失) 1." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://meanvery.github.io/post/20220711/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2022-07-11T00:00:00+00:00" />
<meta property="article:modified_time" content="2022-07-11T00:00:00+00:00" />


  
  <meta itemprop="name" content="An Overview of Computer Vision">
<meta itemprop="description" content="Computer Vision1 Image Classification(图像分类)：将图片按内容分到某一类别 Object Detection(物体检测)：检测出图片中的目标对象 Segmentation(图像分割)：将图片按内容分割 Semantic Segmentation(语义分割)：不同物体分成不同的部分 Instance Segmentation(实例分割)：同一物体要分为不同个体 Image Generation(图像生成) 1 Neural Network 1.1 Forward Propogation(前向传播) The input data should be fed in the forward direction only. 神经元通过加权和与激活函数决定是否进一步传递数据。 常见激活函数：Sigmoid, ReLU, Softmax, Hyperbolic Tangent
1.2 Back Propogation(反向传播) It is the practice of fine-tuning the weights of a neural net based on the error rate obtained in the previous epoch.
1.3 Several Loss Function(损失函数) Mean Absolute Error(平均绝对误差) Mean Square Error(均方误差) Hinge Loss(铰链损失) Cross Entropy Loss(交叉熵损失) 1."><meta itemprop="datePublished" content="2022-07-11T00:00:00+00:00" />
<meta itemprop="dateModified" content="2022-07-11T00:00:00+00:00" />
<meta itemprop="wordCount" content="574">
<meta itemprop="keywords" content="计算机视觉,综述," />
  
  <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="An Overview of Computer Vision"/>
<meta name="twitter:description" content="Computer Vision1 Image Classification(图像分类)：将图片按内容分到某一类别 Object Detection(物体检测)：检测出图片中的目标对象 Segmentation(图像分割)：将图片按内容分割 Semantic Segmentation(语义分割)：不同物体分成不同的部分 Instance Segmentation(实例分割)：同一物体要分为不同个体 Image Generation(图像生成) 1 Neural Network 1.1 Forward Propogation(前向传播) The input data should be fed in the forward direction only. 神经元通过加权和与激活函数决定是否进一步传递数据。 常见激活函数：Sigmoid, ReLU, Softmax, Hyperbolic Tangent
1.2 Back Propogation(反向传播) It is the practice of fine-tuning the weights of a neural net based on the error rate obtained in the previous epoch.
1.3 Several Loss Function(损失函数) Mean Absolute Error(平均绝对误差) Mean Square Error(均方误差) Hinge Loss(铰链损失) Cross Entropy Loss(交叉熵损失) 1."/>

  
  
</head>


  <body class="not-ready" data-menu="true">
    <header class="header">
  
  <p class="logo">
    <a class="site-name" href="https://meanvery.github.io/">Very的计算机学习</a><a class="btn-dark"></a>
  </p>
  

  <script>
    let bodyClx = document.body.classList;
    let btnDark = document.querySelector('.btn-dark');
    let sysDark = window.matchMedia('(prefers-color-scheme: dark)');
    let darkVal = localStorage.getItem('dark');

    let setDark = (isDark) => {
      bodyClx[isDark ? 'add' : 'remove']('dark');
      localStorage.setItem('dark', isDark ? 'yes' : 'no');
    };

    setDark(darkVal ? darkVal === 'yes' : sysDark.matches);
    requestAnimationFrame(() => bodyClx.remove('not-ready'));

    btnDark.addEventListener('click', () => setDark(!bodyClx.contains('dark')));
    sysDark.addEventListener('change', (event) => setDark(event.matches));
  </script>

  
  
  <nav class="menu">
    
    <a class="" href="/about/">关于</a>
    
  </nav>
  

  
  <nav class="social">
    
    <a
      class="github"
      style="--url: url(./github.svg)"
      href="https://github.com/MeanVery"
      target="_blank"
    ></a>
    
  </nav>
  
</header>


    <main class="main">

<article class="post-single">
  <header class="post-title">
    <p>
      
      <time>Jul 11, 2022</time>
      
      
      <span>MeanVery</span>
      
    </p>
    <h1>An Overview of Computer Vision</h1>
  </header>
  <section class="post-content"><h2 id="computer-vision1cite">Computer Vision<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></cite></h2>
<ul>
<li>Image Classification(图像分类)：将图片按内容分到某一类别</li>
<li>Object Detection(物体检测)：检测出图片中的目标对象</li>
<li>Segmentation(图像分割)：将图片按内容分割
<ul>
<li>Semantic Segmentation(语义分割)：不同物体分成不同的部分</li>
<li>Instance Segmentation(实例分割)：同一物体要分为不同个体</li>
</ul>
</li>
<li>Image Generation(图像生成)</li>
</ul>
<h3 id="1-neural-network">1 Neural Network</h3>
<h4 id="11-forward-propogation前向传播">1.1 Forward Propogation(前向传播)</h4>
<p>The input data should be fed in the forward direction only.
神经元通过加权和与激活函数决定是否进一步传递数据。
<br>常见激活函数：Sigmoid, ReLU, Softmax, Hyperbolic Tangent</p>
<h4 id="12-back-propogation反向传播">1.2 Back Propogation(反向传播)</h4>
<p>It is the practice of fine-tuning the weights of a neural net based on the error rate obtained in the previous epoch.</p>
<h4 id="13-several-loss-function损失函数">1.3 Several Loss Function(损失函数)</h4>
<ul>
<li>Mean Absolute Error(平均绝对误差)</li>
<li>Mean Square Error(均方误差)</li>
<li>Hinge Loss(铰链损失)</li>
<li>Cross Entropy Loss(交叉熵损失)</li>
</ul>
<h4 id="14-prevent-overfitting防止过拟合">1.4 Prevent Overfitting(防止过拟合)</h4>
<h4 id="15-hyperparameter-tuning超参数优化">1.5 Hyperparameter Tuning(超参数优化)</h4>
<ul>
<li>Grid Search(网格搜索)</li>
<li>Random Search(随机搜索)
<br>This works best under the assumption that not all hyperparameters are equally important.</li>
<li>Bayesian Optimization(贝叶斯优化)
<br>It belongs to a class of sequential model-based optimization(SMBO) algorithms that allow for one to use the results of our previous iteration to improve our sampling method of the next experiments.</li>
</ul>
<h4 id="16-cnn-architecturescnn-架构">1.6 CNN Architectures(CNN 架构)</h4>
<h4 id="17-attention注意力机制">1.7 Attention(注意力机制)</h4>
<p>It is a method that tries to enhance the important parts while fading out the non-relevant information.</p>
<ul>
<li>
<p>Number of Sequences
<br>In Distinctive Attention models, the candidate state and query states from encoder-decoder belong to two distinct input and output sequences. Co-attention models have multiple input sequences at the same time. Attention weights are learned based on all the input sequences. Co-attention models can be used for image inputs. In recommendations and text classification problems, the input is in sequence, but the output is not a sequence. For such issues, Self-Attention is used where candidate state and query state both belongs to the same input sequence.</p>
</li>
<li>
<p>Number of Abstractions
<br>The attention models having single level abstractions compute attention weights just for the original input sequence. The multi-level abstraction attention models apply attention on multiple levels of abstraction on the input sequence. In this type of attention, the lower abstraction level’s context vector becomes the query state for high-level abstraction. Such models can be classified further as top-down or bottom-up models.</p>
</li>
<li>
<p>Number of Positions
<br>In this category of attention models, the models are further classified based on the input sequence position where attention function is calculated. In Soft Attention models, the context vector is computed using the weighted average of all hidden stages of the input sequence. These models enable the neural network to learn from backpropagation efficiently. However, it leads to quadratic computational loss. In hard attention models, the context vector is built using hidden states which are stochastically sampled in the input sequence. The global attention model is similar to the soft attention model, whereas the local attention model is midway between soft and hard attention mechanisms.</p>
</li>
<li>
<p>Number of Representations
<br>The Multi-Representational attention models determine different aspects of the input sequence through multiple feature representations. The weight importance is assigned to these multiple feature representations using attention to decide which aspects are most relevant. In multi-dimensional attention, the weights are generated to determine the relevance of each dimension of the input sequence. These models are used for natural language processing applications.</p>
</li>
</ul>
<h3 id="2-image-processing">2 Image Processing</h3>
<h4 id="21-read-image">2.1 Read Image</h4>
<p>We store the path to our image dataset into a variable then we create a function to load folders containing images into arrays.</p>
<h4 id="22-resize-image">2.2 Resize Image</h4>
<h4 id="23-denoise">2.3 Denoise</h4>
<p>Gaussian Blur</p>
<h4 id="24-gradiant-calculation">2.4 Gradiant Calculation</h4>
<p>Detect the edge intensity and direction</p>
<h4 id="25-non-maximum-suppression">2.5 Non-Maximum Suppression</h4>
<p>Make the final image have thin edges</p>
<h4 id="26-double-threshold">2.6 Double Threshold</h4>
<ul>
<li>High threshold is to identify the strong pixels</li>
<li>Low threshold is to identify the non-relevant pixels</li>
</ul>
<h3 id="3-object-detection">3 Object Detection</h3>
<h4 id="31-region-cnn-model-family">3.1 Region-CNN Model Family</h4>
<ul>
<li>R-CNN</li>
<li>Fast R-CNN</li>
<li>Faster R-CNN</li>
<li>Mask R-CNN</li>
</ul>
<h4 id="32-abbr-titleyou-only-look-onceyoloabbr-model-family">3.2 <abbr title="You Only Look Once">YOLO</abbr> Model Family</h4>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>本文内容源自于CSDN博主[Billy1900 的原创文章] 
(<a href="https://blog.csdn.net/Billy1900/article/details/118227427">https://blog.csdn.net/Billy1900/article/details/118227427</a>)&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
</section>

  
  
  <footer class="post-tags">
     
    <a href="https://meanvery.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89">计算机视觉</a>
     
    <a href="https://meanvery.github.io/tags/%E7%BB%BC%E8%BF%B0">综述</a>
    
  </footer>
  

  
  
  
  <nav class="post-nav">
    
    <a class="prev" href="https://meanvery.github.io/post/seminar-20220820/"><span>←</span><span>Seminar-20220820</span></a>
     
    <a class="next" href="https://meanvery.github.io/post/format_rules/"><span>format guide</span><span>→</span></a>
    
  </nav>
  

  
  
</article>

</main>

    <footer class="footer">
  <p>&copy; 2022 <a href="https://meanvery.github.io/">Very的计算机学习</a></p>
  <p>Powered by <a href="https://gohugo.io/" rel="noopener" target="_blank">Hugo️️</a>️</p>
  <p>
    <a href="https://github.com/nanxiaobei/hugo-paper" rel="noopener" target="_blank">Paper 5.1</a>
  </p>
</footer>

  </body>
</html>
